{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              band_1  \\\n",
      "0  [-27.878360999999998, -27.15416, -28.668615, -...   \n",
      "1  [-12.242375, -14.920304999999999, -14.920363, ...   \n",
      "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
      "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
      "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
      "\n",
      "                                              band_2        id inc_angle  \\\n",
      "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913   43.9239   \n",
      "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd   38.1562   \n",
      "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0   45.2859   \n",
      "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18   43.8306   \n",
      "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4   35.6256   \n",
      "\n",
      "   is_iceberg  \n",
      "0           0  \n",
      "1           0  \n",
      "2           1  \n",
      "3           0  \n",
      "4           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_json('../input/train.json')\n",
    "test_df = pd.read_json('../input/test.json')\n",
    "print(train_df.head())\n",
    "\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.ndimage import laplace, sobel\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/feature/_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw feats\n"
     ]
    }
   ],
   "source": [
    "def get_img_feat(img_org):\n",
    "    img = img_org.ravel()\n",
    "    feats = [np.mean(img),np.std(img),np.median(img),np.max(img),np.min(img)]\n",
    "    return feats\n",
    "\n",
    "from skimage.feature import hog\n",
    "def get_hog_feat(img):\n",
    "    hog_image = hog(img)\n",
    "    return hog_image.ravel()\n",
    "\n",
    "def get_other_feat(df):\n",
    "    band1,band2,band3,band4,angs = [],[],[],[],[]\n",
    "    for i, row in df.iterrows():\n",
    "        tmp_feat = []\n",
    "        img1 = np.array(row['band_1']).astype('float32').reshape(75,75)\n",
    "        img2 = np.array(row['band_2']).astype('float32').reshape(75,75)\n",
    "        \n",
    "        if row['inc_angle'] == 'na':\n",
    "            ang = -1\n",
    "        else:\n",
    "            ang = float(row['inc_angle'])\n",
    "            \n",
    "        img3 = (img1+img2)*ang/2.0\n",
    "        img = np.dstack([img1,img2,img3])\n",
    "        #print(img.shape)\n",
    "\n",
    "        tmp_feat = [ang] + get_img_feat(img1) + get_img_feat(img2) + get_img_feat(img3)\n",
    "        tmp_feat += list(get_hog_feat(img1)) + list(get_hog_feat(img2))\n",
    "        angs.append(tmp_feat)\n",
    "    return angs\n",
    "        \n",
    "        \n",
    "\n",
    "a_angs = get_other_feat(train_df)\n",
    "b_angs = get_other_feat(test_df)\n",
    "print('raw feats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7954\n"
     ]
    }
   ],
   "source": [
    "print(len(a_angs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "comp = 100\n",
    "pca_b1 = decomposition.PCA(n_components=comp, whiten=True, random_state=15)\n",
    "train_feat = pca_b1.fit_transform(np.array(a_angs))\n",
    "test_feat = pca_b1.transform(np.array(b_angs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca done (1604, 100)\n"
     ]
    }
   ],
   "source": [
    "print('pca done',train_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 100) (8424, 100)\n"
     ]
    }
   ],
   "source": [
    "print(train_feat.shape,test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = train_df.is_iceberg.values\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "def cv_feat(model_f,fold_cnt=3,rnd=1,params={}):\n",
    "    train_pred, test_pred = np.zeros((1604,1)),np.zeros((8424,1))\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=True, random_state=2*rnd)\n",
    "    avg_train_l,avg_val_l = 0,0\n",
    "    print(model_f(**params))\n",
    "    for train_index, test_index in kf.split(train_feat):\n",
    "        curr_x,curr_y = train_feat[train_index],y[train_index]\n",
    "        val_x,val_y = train_feat[test_index],y[test_index]\n",
    "        \n",
    "        model = model_f(**params)\n",
    "        model.fit(curr_x,curr_y)\n",
    "        \n",
    "        curr_train_pred = model.predict_proba(curr_x)\n",
    "        curr_val_pred = model.predict_proba(val_x)\n",
    "        train_pred[test_index] = curr_val_pred[:,1].reshape(-1,1)\n",
    "        curr_test_pred = model.predict_proba(test_feat)/fold_cnt\n",
    "        test_pred = test_pred + curr_test_pred[:,1].reshape(-1,1)\n",
    "        \n",
    "        loss1 = log_loss(curr_y,curr_train_pred)\n",
    "        loss2 = log_loss(val_y,curr_val_pred)\n",
    "        avg_train_l += loss1/fold_cnt\n",
    "        avg_val_l += loss2/fold_cnt\n",
    "        print('this fold train loss',loss1,'val loss',loss2)\n",
    "        print('============================')\n",
    "    print('all avg',avg_train_l,avg_val_l)\n",
    "    return train_pred,test_pred\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "this fold train loss 0.269593680574 val loss 0.402170416805\n",
      "============================\n",
      "this fold train loss 0.260024213121 val loss 0.434825632644\n",
      "============================\n",
      "this fold train loss 0.284566432634 val loss 0.320029915382\n",
      "============================\n",
      "this fold train loss 0.271157241294 val loss 0.389175990056\n",
      "============================\n",
      "this fold train loss 0.284033309107 val loss 0.332814600544\n",
      "============================\n",
      "all avg 0.273874975346 0.375803311086\n"
     ]
    }
   ],
   "source": [
    "# lr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_train,lr_pred = cv_feat(LogisticRegression,\n",
    "                           fold_cnt=5,\n",
    "                           params={'C':2.0,'max_iter':100},rnd=2)\n",
    "import pickle\n",
    "with open('../features/other_model_lr8.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "this fold train loss 0.308165266592 val loss 0.483434186804\n",
      "============================\n",
      "this fold train loss 0.301766994495 val loss 0.461749482516\n",
      "============================\n",
      "this fold train loss 0.307211242299 val loss 0.436702338151\n",
      "============================\n",
      "this fold train loss 0.314233922706 val loss 0.465284021533\n",
      "============================\n",
      "this fold train loss 0.309547675901 val loss 0.472849105451\n",
      "============================\n",
      "all avg 0.308185020399 0.464003826891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "tmp_params = {\n",
    "    'n_estimators':25,\n",
    "    'max_depth':8,\n",
    "    'random_state':42\n",
    "}\n",
    "lr_train,lr_pred = cv_feat(RandomForestClassifier,fold_cnt=5,params=tmp_params)\n",
    "with open('../features/other_model_rf8.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tmp_params = {\n",
    "#     'n_estimators':20,\n",
    "#     'learning_rate':0.01,\n",
    "#     'random_state':42\n",
    "# }\n",
    "# lr_train,lr_pred = cv_feat(AdaBoostClassifier,fold_cnt=5,params=tmp_params)\n",
    "# with open('../features/other_model_ada4.pkl','wb') as fout:\n",
    "#     pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=230,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "this fold train loss 0.0639251411234 val loss 0.339664064947\n",
      "============================\n",
      "this fold train loss 0.0575079253514 val loss 0.326549729855\n",
      "============================\n",
      "this fold train loss 0.0614482500407 val loss 0.295933624057\n",
      "============================\n",
      "this fold train loss 0.0600693129791 val loss 0.311650222212\n",
      "============================\n",
      "this fold train loss 0.054895951967 val loss 0.349832157115\n",
      "============================\n",
      "all avg 0.0595693162923 0.324725959637\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    'n_estimators':230,\n",
    "    'learning_rate':0.1,\n",
    "    'random_state':42,\n",
    "    'subsample':1.0,\n",
    "    'min_samples_leaf':1,\n",
    "    'max_depth':3\n",
    "}\n",
    "lr_train,lr_pred = cv_feat(GradientBoostingClassifier,fold_cnt=5,\n",
    "                           params=tmp_params,\n",
    "                           rnd=1\n",
    "                          )\n",
    "with open('../features/other_model_gbrt8.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "       min_child_weight=1, missing=None, n_estimators=50, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this fold train loss 0.263670170265 val loss 0.388673091864\n",
      "============================\n",
      "this fold train loss 0.271266560518 val loss 0.376387626559\n",
      "============================\n",
      "this fold train loss 0.279249230797 val loss 0.34037718572\n",
      "============================\n",
      "this fold train loss 0.270698858087 val loss 0.362372293241\n",
      "============================\n",
      "this fold train loss 0.266488230815 val loss 0.371144251668\n",
      "============================\n",
      "all avg 0.270274610096 0.367790889811\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "tmp_params = {\n",
    "    'n_estimators':50,\n",
    "    'colsample_bytree':1,\n",
    "    'min_child_weight':1,\n",
    "    'learning_rate':0.1,\n",
    "\n",
    "    \n",
    "}\n",
    "lr_train,lr_pred = cv_feat(XGBClassifier,fold_cnt=5,\n",
    "                           params=tmp_params,rnd=1)\n",
    "with open('../features/other_model_xgb8.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ed4c29283952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m tmp_params = {\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'n_estimators'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#'min_child_samples':20,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "tmp_params = {\n",
    "    'max_depth':3,  \n",
    "    'n_estimators':500,\n",
    "    #'min_child_samples':20,\n",
    "    #'reg_lambda':0.1,\n",
    "}\n",
    "lr_train,lr_pred = cv_feat(LGBMClassifier,fold_cnt=5,params=tmp_params)\n",
    "with open('../features/other_model_lgb8.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
