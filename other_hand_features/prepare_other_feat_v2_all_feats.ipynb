{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              band_1  \\\n",
      "0  [-27.878360999999998, -27.15416, -28.668615, -...   \n",
      "1  [-12.242375, -14.920304999999999, -14.920363, ...   \n",
      "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
      "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
      "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
      "\n",
      "                                              band_2        id inc_angle  \\\n",
      "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913   43.9239   \n",
      "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd   38.1562   \n",
      "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0   45.2859   \n",
      "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18   43.8306   \n",
      "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4   35.6256   \n",
      "\n",
      "   is_iceberg  \n",
      "0           0  \n",
      "1           0  \n",
      "2           1  \n",
      "3           0  \n",
      "4           0  \n",
      "                                              band_1  \\\n",
      "0  [-15.863251, -15.201077, -17.887735, -19.17248...   \n",
      "1  [-26.058969497680664, -26.058969497680664, -26...   \n",
      "2  [-14.14109992980957, -15.064241409301758, -17....   \n",
      "3  [-12.167478, -13.706167, -16.54837, -13.572674...   \n",
      "4  [-23.37459373474121, -26.02718162536621, -28.1...   \n",
      "\n",
      "                                              band_2        id  inc_angle  \n",
      "0  [-21.629612, -21.142353, -23.908337, -28.34524...  5941774d  34.966400  \n",
      "1  [-25.754207611083984, -25.754207611083984, -25...  4023181e  32.615072  \n",
      "2  [-14.74563980102539, -14.590410232543945, -14....  b20200e4  37.505433  \n",
      "3  [-24.32222, -26.375538, -24.096739, -23.8769, ...  e7f018bb  34.473900  \n",
      "4  [-25.72234344482422, -27.011577606201172, -23....  4371c8c3  43.918874  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_json('../input/train.json')\n",
    "test_df = pd.read_json('../input/test.json')\n",
    "print(train_df.head())\n",
    "print(test_df.head())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.ndimage import laplace, sobel\n",
    "from scipy.stats import kurtosis, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw feats\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_img_feat(img):\n",
    "    feats = [np.mean(img),np.std(img),np.median(img),np.max(img),np.min(img)]\n",
    "    return feats\n",
    "    \n",
    "def get_other_feat(df):\n",
    "    feats = []\n",
    "    band1,band2,band3,band4 = [],[],[],[]\n",
    "    for i, row in df.iterrows():\n",
    "        tmp_feat = []\n",
    "        img1 = np.array(row['band_1']).reshape(75, 75).astype('float32')\n",
    "        img2 = np.array(row['band_2']).reshape(75, 75).astype('float32')\n",
    "        if row['inc_angle'] == 'na':\n",
    "            ang = -1\n",
    "        else:\n",
    "            ang = float(row['inc_angle'])\n",
    "        img3 = (img1+img2)*ang/2.0\n",
    "        img4 = (img1-img2)*ang/2.0\n",
    "        band1.append(img1.ravel())\n",
    "        band2.append(img2.ravel())\n",
    "        band3.append(img3.ravel())\n",
    "        band4.append(img4.ravel())\n",
    "            \n",
    "        # base\n",
    "        st_trans = get_img_feat(img1) + get_img_feat(img2) + [ang] \n",
    "        tmp_feat += st_trans\n",
    "        tmp_feat += [x * y for x, y in combinations(st_trans, 2)]\n",
    "        tmp_feat += [x + y for x, y in combinations(st_trans, 2)]\n",
    "        tmp_feat += [x - y for x, y in combinations(st_trans, 2)]\n",
    "        \n",
    "        # lap\n",
    "        lap_1 = laplace(img1, mode='reflect', cval=0.0)\n",
    "        lap_2 = laplace(img2, mode='reflect', cval=0.0)\n",
    "        st_trans = get_img_feat(lap_1) + get_img_feat(lap_2)\n",
    "        \n",
    "        # sob\n",
    "        sob1 = sobel(img1, axis=0, mode='reflect', cval=0.0)\n",
    "        sob2 = sobel(img1, axis=1, mode='reflect', cval=0.0)\n",
    "        sob3 = sobel(img2, axis=0, mode='reflect', cval=0.0)\n",
    "        sob4 = sobel(img2, axis=1, mode='reflect', cval=0.0)\n",
    "        st_trans = st_trans + get_img_feat(sob1) + get_img_feat(sob2) + get_img_feat(sob3) + get_img_feat(sob4)\n",
    "        tmp_feat += st_trans\n",
    "        \n",
    "        # hist\n",
    "        hist = list(np.histogram(img1, bins=20)[0])\n",
    "        tmp_feat += hist\n",
    "        tmp_feat += [np.std(hist), np.max(hist), np.median(hist), (np.max(hist) - np.median(hist))]\n",
    "        \n",
    "        hist = list(np.histogram(img2, bins=20)[0])\n",
    "        tmp_feat += hist\n",
    "        tmp_feat += [np.std(hist), np.max(hist), np.median(hist), (np.max(hist) - np.median(hist))]\n",
    "        \n",
    "        tmp_feat += get_img_feat(img3) + get_img_feat(img4)\n",
    "        feats.append(tmp_feat)\n",
    "    return band1,band2,band3,band4,feats\n",
    "\n",
    "a_band1,a_band2,a_band3,a_band4,a_angs = get_other_feat(train_df)\n",
    "b_band1,b_band2,b_band3,b_band4,b_angs = get_other_feat(test_df)\n",
    "print('raw feats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca done (1604, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "comp = 50\n",
    "pca_b1 = decomposition.PCA(n_components=comp, whiten=True, random_state=15)\n",
    "pca_b2 = decomposition.PCA(n_components=comp, whiten=True, random_state=16)\n",
    "pca_b3 = decomposition.PCA(n_components=comp, whiten=True, random_state=17)\n",
    "pca_b4 = decomposition.PCA(n_components=comp, whiten=True, random_state=18)\n",
    "\n",
    "a_band1_feat = pca_b1.fit_transform(np.array(a_band1))\n",
    "a_band2_feat = pca_b2.fit_transform(np.array(a_band2))\n",
    "a_band3_feat = pca_b3.fit_transform(np.array(a_band3))\n",
    "a_band4_feat = pca_b4.fit_transform(np.array(a_band4))\n",
    "\n",
    "b_band1_feat = pca_b1.transform(np.array(b_band1))\n",
    "b_band2_feat = pca_b2.transform(np.array(b_band2))\n",
    "b_band3_feat = pca_b3.transform(np.array(b_band3))\n",
    "b_band4_feat = pca_b4.transform(np.array(b_band4))\n",
    "\n",
    "print('pca done',a_band1_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca done (1604, 60)\n"
     ]
    }
   ],
   "source": [
    "a_bands = np.hstack([a_band1,a_band2,a_band3,a_band4])\n",
    "b_bands = np.hstack([b_band1,b_band2,b_band3,b_band4])\n",
    "pca_bx = decomposition.PCA(n_components=60, whiten=True, random_state=15)\n",
    "a_band_feat = pca_bx.fit_transform(np.array(a_bands))\n",
    "b_band_feat = pca_bx.transform(np.array(b_bands))\n",
    "print('pca done',a_band_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 524) (8424, 524)\n"
     ]
    }
   ],
   "source": [
    "train_feat = np.hstack([a_band1_feat,a_band2_feat,a_band3_feat,a_band4_feat,a_band_feat,np.array(a_angs)])\n",
    "test_feat = np.hstack([b_band1_feat,b_band2_feat,b_band3_feat,b_band4_feat,b_band_feat,np.array(b_angs)])\n",
    "\n",
    "print(train_feat.shape,test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('other_feat_v2.pkl','wb') as fout:\n",
    "    pickle.dump([train_feat,test_feat],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = train_df.is_iceberg.values\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "def cv_feat(model_f,fold_cnt=3,rnd=1,params={}):\n",
    "    train_pred, test_pred = np.zeros((1604,1)),np.zeros((8424,1))\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=True, random_state=2*rnd)\n",
    "    avg_train_l,avg_val_l = 0,0\n",
    "    print(model_f(**params))\n",
    "    for train_index, test_index in kf.split(train_feat):\n",
    "        curr_x,curr_y = train_feat[train_index],y[train_index]\n",
    "        val_x,val_y = train_feat[test_index],y[test_index]\n",
    "        \n",
    "        model = model_f(**params)\n",
    "        model.fit(curr_x,curr_y)\n",
    "        \n",
    "        curr_train_pred = model.predict_proba(curr_x)\n",
    "        curr_val_pred = model.predict_proba(val_x)\n",
    "        train_pred[test_index] = curr_val_pred[:,1].reshape(-1,1)\n",
    "        curr_test_pred = model.predict_proba(test_feat)/fold_cnt\n",
    "        test_pred = test_pred + curr_test_pred[:,1].reshape(-1,1)\n",
    "        \n",
    "        loss1 = log_loss(curr_y,curr_train_pred)\n",
    "        loss2 = log_loss(val_y,curr_val_pred)\n",
    "        avg_train_l += loss1/fold_cnt\n",
    "        avg_val_l += loss2/fold_cnt\n",
    "        print('this fold train loss',loss1,'val loss',loss2)\n",
    "        print('============================')\n",
    "    print('all avg',avg_train_l,avg_val_l)\n",
    "    return train_pred,test_pred\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=30, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "this fold train loss 0.207241946492 val loss 0.286242288204\n",
      "============================\n",
      "this fold train loss 0.21964769911 val loss 0.320364011012\n",
      "============================\n",
      "this fold train loss 0.225364154965 val loss 0.279900130972\n",
      "============================\n",
      "this fold train loss 0.214315087908 val loss 0.290438618302\n",
      "============================\n",
      "this fold train loss 0.20934601504 val loss 0.29697624655\n",
      "============================\n",
      "all avg 0.215182980703 0.294784259008\n"
     ]
    }
   ],
   "source": [
    "# lr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_train,lr_pred = cv_feat(LogisticRegression,\n",
    "                           fold_cnt=5,\n",
    "                           params={'C':2.0,'max_iter':30},rnd=2)\n",
    "import pickle\n",
    "with open('../2nd_features/other_model_lr7.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "this fold train loss 0.129296674685 val loss 0.30854446922\n",
      "============================\n",
      "this fold train loss 0.124918438728 val loss 0.306518453839\n",
      "============================\n",
      "this fold train loss 0.126731662737 val loss 0.273198313406\n",
      "============================\n",
      "this fold train loss 0.122876105306 val loss 0.306134897742\n",
      "============================\n",
      "this fold train loss 0.121726111663 val loss 0.307918551165\n",
      "============================\n",
      "all avg 0.125109798624 0.300462937074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "tmp_params = {\n",
    "    'n_estimators':200,\n",
    "    'max_depth':10,\n",
    "    'random_state':42\n",
    "}\n",
    "lr_train,lr_pred = cv_feat(RandomForestClassifier,fold_cnt=5,params=tmp_params)\n",
    "with open('../2nd_features/other_model_rf7.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "              presort='auto', random_state=42, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "this fold train loss 0.0283999852997 val loss 0.258734573524\n",
      "============================\n",
      "this fold train loss 0.0307841721068 val loss 0.193073629645\n",
      "============================\n",
      "this fold train loss 0.0278715195163 val loss 0.186784206681\n",
      "============================\n",
      "this fold train loss 0.0292057013911 val loss 0.264800696067\n",
      "============================\n",
      "this fold train loss 0.0244842934591 val loss 0.255111447009\n",
      "============================\n",
      "all avg 0.0281491343546 0.231700910585\n"
     ]
    }
   ],
   "source": [
    "tmp_params = {\n",
    "    'n_estimators':200,\n",
    "    'learning_rate':0.1,\n",
    "    'random_state':42,\n",
    "    'subsample':1.0,\n",
    "    'min_samples_leaf':1,\n",
    "    'max_depth':3\n",
    "}\n",
    "lr_train,lr_pred = cv_feat(GradientBoostingClassifier,fold_cnt=5,\n",
    "                           params=tmp_params,\n",
    "                           rnd=1\n",
    "                          )\n",
    "with open('../2nd_features/other_model_gbrt7.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=200,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "this fold train loss 0.021899639521 val loss 0.236528103225\n",
      "============================\n",
      "this fold train loss 0.0249325548873 val loss 0.183005113788\n",
      "============================\n",
      "this fold train loss 0.0252389829969 val loss 0.185084255211\n",
      "============================\n",
      "this fold train loss 0.0226569409003 val loss 0.237792021364\n",
      "============================\n",
      "this fold train loss 0.0223828670976 val loss 0.254871491637\n",
      "============================\n",
      "all avg 0.0234221970806 0.219456197045\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "tmp_params = {\n",
    "    'n_estimators':200,\n",
    "    'colsample_bytree':1,\n",
    "    'min_child_weight':1,\n",
    "    'learning_rate':0.1,\n",
    "\n",
    "    \n",
    "}\n",
    "lr_train,lr_pred = cv_feat(XGBClassifier,fold_cnt=5,\n",
    "                           params=tmp_params,rnd=1)\n",
    "with open('../2nd_features/other_model_xgb7.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.1,\n",
      "        max_bin=255, max_depth=3, min_child_samples=20,\n",
      "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=200,\n",
      "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
      "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "        subsample_for_bin=200000, subsample_freq=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this fold train loss 0.0173393599209 val loss 0.238832237991\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this fold train loss 0.0198665303243 val loss 0.192208286688\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this fold train loss 0.0194814666523 val loss 0.178429272092\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this fold train loss 0.0171481851864 val loss 0.237144395837\n",
      "============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this fold train loss 0.0169822979942 val loss 0.251440546473\n",
      "============================\n",
      "all avg 0.0181635680156 0.219610947816\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "tmp_params = {\n",
    "    'max_depth':3,  \n",
    "    'n_estimators':200,\n",
    "    #'min_child_samples':20,\n",
    "    #'reg_lambda':0.1,\n",
    "}\n",
    "lr_train,lr_pred = cv_feat(LGBMClassifier,fold_cnt=5,params=tmp_params)\n",
    "with open('../2nd_features/other_model_lgb7.pkl','wb') as fout:\n",
    "    pickle.dump([lr_train,lr_pred],fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
