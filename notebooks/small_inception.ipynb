{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "train_df = pd.read_json('../input/train.json')\n",
    "test_df = pd.read_json('../input/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75, 3) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_image(df):\n",
    "    '''Create 3-channel 'images'. Return rescale-normalised images.'''\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Formulate the bands as 75x75 arrays\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "\n",
    "        # Rescale\n",
    "        r = (band_1 - band_1.min()) / (band_1.max() - band_1.min())\n",
    "        g = (band_2 - band_2.min()) / (band_2.max() - band_2.min())\n",
    "        b = (band_3 - band_3.min()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        rgb = np.dstack((r, g, b))\n",
    "        images.append(rgb)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "train_x = get_image(train_df)\n",
    "test_x = get_image(test_df)\n",
    "\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = train_df.is_iceberg.values\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rot_aut(Xtr,ytr):\n",
    "    # aug on train\n",
    "    data_cnt = len(ytr)\n",
    "    print(data_cnt)\n",
    "    aug_X = []\n",
    "    aug_y = []\n",
    "\n",
    "    for i in range(data_cnt):\n",
    "        img = Xtr[i]\n",
    "        tmp_y = ytr[i]\n",
    "\n",
    "        # org img\n",
    "        aug_X.append(img)\n",
    "        aug_y.append(tmp_y)\n",
    "\n",
    "        # flip\n",
    "        tmp_img = np.fliplr(img)\n",
    "        aug_X.append(tmp_img)\n",
    "        aug_y.append(tmp_y)\n",
    "\n",
    "        tmp_img = np.flipud(img)\n",
    "        aug_X.append(tmp_img)\n",
    "        aug_y.append(tmp_y)\n",
    "\n",
    "        tmp_img = np.rot90(img)\n",
    "        aug_X.append(tmp_img)\n",
    "        aug_y.append(tmp_y)\n",
    "    return np.array(aug_X),np.array(aug_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 37, 37, 32)   864         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 37, 37, 32)   96          conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 37, 37, 32)   0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 35, 35, 32)   9216        activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 35, 35, 32)   96          conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 35, 35, 32)   0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 35, 35, 64)   18432       activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 35, 35, 64)   192         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 35, 35, 64)   0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 17, 17, 64)   0           activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 17, 17, 64)   4096        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 17, 17, 64)   192         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 17, 17, 64)   0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 15, 15, 64)   36864       activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 15, 15, 64)   192         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 15, 15, 64)   0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 7, 7, 64)     0           activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 7, 7, 64)     4096        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 7, 7, 64)     192         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 7, 7, 64)     0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 7, 7, 48)     3072        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 7, 7, 64)     36864       activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 7, 7, 48)     144         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 7, 7, 64)     192         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 7, 7, 48)     0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 7, 7, 64)     0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 7, 7, 64)     0           max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 7, 7, 64)     4096        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 7, 7, 64)     76800       activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 7, 7, 64)     36864       activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 7, 7, 32)     2048        average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 7, 7, 64)     192         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 7, 7, 64)     192         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 7, 7, 64)     192         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 7, 7, 32)     96          conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 7, 7, 64)     0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 7, 7, 64)     0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 7, 7, 64)     0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 7, 7, 32)     0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 7, 7, 224)    0           activation_162[0][0]             \n",
      "                                                                 activation_164[0][0]             \n",
      "                                                                 activation_167[0][0]             \n",
      "                                                                 activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 7, 7, 64)     14336       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 7, 7, 64)     192         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 7, 7, 64)     0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 7, 7, 48)     10752       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 7, 7, 64)     36864       activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 7, 7, 48)     144         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 7, 7, 64)     192         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 7, 7, 48)     0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 7, 7, 64)     0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePo (None, 7, 7, 224)    0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 7, 7, 64)     14336       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 7, 7, 64)     76800       activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 7, 7, 64)     36864       activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 7, 7, 64)     14336       average_pooling2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 7, 7, 64)     192         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 7, 7, 64)     192         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 7, 7, 64)     192         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 7, 7, 64)     192         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 7, 7, 64)     0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 7, 7, 64)     0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 7, 7, 64)     0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 7, 7, 64)     0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 7, 7, 256)    0           activation_169[0][0]             \n",
      "                                                                 activation_171[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "                                                                 activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 7, 7, 64)     16384       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 7, 7, 64)     192         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 7, 7, 64)     0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 7, 7, 48)     12288       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 7, 7, 64)     36864       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 7, 7, 48)     144         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 7, 7, 64)     192         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 7, 7, 48)     0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 7, 7, 64)     0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_23 (AveragePo (None, 7, 7, 256)    0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 7, 7, 64)     16384       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 7, 7, 64)     76800       activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 7, 7, 64)     36864       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 7, 7, 64)     16384       average_pooling2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 7, 7, 64)     192         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 7, 7, 64)     192         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 7, 7, 64)     192         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 7, 7, 64)     192         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 7, 7, 64)     0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 7, 7, 64)     0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 7, 7, 64)     0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 7, 7, 64)     0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 7, 7, 256)    0           activation_176[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "                                                                 activation_181[0][0]             \n",
      "                                                                 activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 4, 4, 64)     147520      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 1024)         0           conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 256)          262400      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            257         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,064,305\n",
      "Trainable params: 1,061,265\n",
      "Non-trainable params: 3,040\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input,AveragePooling2D,GlobalMaxPooling2D,GlobalAveragePooling2D,Flatten\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              num_row,\n",
    "              num_col,\n",
    "              padding='same',\n",
    "              strides=(1, 1),\n",
    "              name=None):\n",
    "\n",
    "    if name is not None:\n",
    "        bn_name = name + '_bn'\n",
    "        conv_name = name + '_conv'\n",
    "    else:\n",
    "        bn_name = None\n",
    "        conv_name = None\n",
    "    bn_axis = 3\n",
    "    x = Conv2D(\n",
    "        filters, (num_row, num_col),\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        use_bias=False,\n",
    "        name=conv_name)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n",
    "    x = Activation('relu', name=name)(x)\n",
    "    return x\n",
    "\n",
    "def create_model():\n",
    "    img_input = Input(shape=(75,75,3))\n",
    "    channel_axis = 3\n",
    "    \n",
    "    # bn\n",
    "    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding='valid')\n",
    "    x = conv2d_bn(x, 32, 3, 3, padding='valid')\n",
    "    x = conv2d_bn(x, 64, 3, 3)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv2d_bn(x, 64, 1, 1, padding='valid')\n",
    "    x = conv2d_bn(x, 64, 3, 3, padding='valid')\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    # mixed 0, 1, 2: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 64, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 64, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed0')\n",
    "    \n",
    "    # mixed 1: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 64, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 64, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed1')\n",
    "    \n",
    "    # mixed 2: 35 x 35 x 256\n",
    "    branch1x1 = conv2d_bn(x, 64, 1, 1)\n",
    "\n",
    "    branch5x5 = conv2d_bn(x, 48, 1, 1)\n",
    "    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n",
    "\n",
    "    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 64, 3, 3)\n",
    "    branch3x3dbl = conv2d_bn(branch3x3dbl, 64, 3, 3)\n",
    "\n",
    "    branch_pool = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n",
    "    x = layers.concatenate(\n",
    "        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n",
    "        axis=channel_axis,\n",
    "        name='mixed2')\n",
    "\n",
    "    x = Conv2D(64, 3, strides=2, padding='same',activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(img_input, x, name='inception_v3')\n",
    "print('model model')\n",
    "\n",
    "test_m = create_model()\n",
    "test_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.30480, saving model to best_m.h5\n",
      " - 21s - loss: 0.4451 - acc: 0.7833 - val_loss: 1.3048 - val_acc: 0.5087\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 9s - loss: 0.3178 - acc: 0.8554 - val_loss: 3.0415 - val_acc: 0.5087\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 1.30480 to 1.13509, saving model to best_m.h5\n",
      " - 10s - loss: 0.2693 - acc: 0.8778 - val_loss: 1.1351 - val_acc: 0.5112\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 1.13509 to 0.33969, saving model to best_m.h5\n",
      " - 10s - loss: 0.2497 - acc: 0.8857 - val_loss: 0.3397 - val_acc: 0.8529\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.33969 to 0.31383, saving model to best_m.h5\n",
      " - 10s - loss: 0.2246 - acc: 0.9059 - val_loss: 0.3138 - val_acc: 0.8379\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.31383 to 0.24113, saving model to best_m.h5\n",
      " - 9s - loss: 0.1923 - acc: 0.9165 - val_loss: 0.2411 - val_acc: 0.9002\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 9s - loss: 0.1679 - acc: 0.9320 - val_loss: 0.2693 - val_acc: 0.8753\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 10s - loss: 0.1417 - acc: 0.9412 - val_loss: 0.2785 - val_acc: 0.8703\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 9s - loss: 0.1060 - acc: 0.9586 - val_loss: 0.4036 - val_acc: 0.8529\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 9s - loss: 0.0890 - acc: 0.9667 - val_loss: 0.2790 - val_acc: 0.8903\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 9s - loss: 0.0923 - acc: 0.9684 - val_loss: 0.3175 - val_acc: 0.9027\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 9s - loss: 0.0586 - acc: 0.9807 - val_loss: 0.4129 - val_acc: 0.8579\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 9s - loss: 0.0663 - acc: 0.9761 - val_loss: 0.3702 - val_acc: 0.8803\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 9s - loss: 0.0594 - acc: 0.9800 - val_loss: 2.6324 - val_acc: 0.5736\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 9s - loss: 0.0481 - acc: 0.9838 - val_loss: 0.4081 - val_acc: 0.8853\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 9s - loss: 0.0408 - acc: 0.9865 - val_loss: 0.4927 - val_acc: 0.8529\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 9s - loss: 0.0403 - acc: 0.9875 - val_loss: 0.8300 - val_acc: 0.8229\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 9s - loss: 0.0395 - acc: 0.9869 - val_loss: 0.3966 - val_acc: 0.8978\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 9s - loss: 0.0294 - acc: 0.9892 - val_loss: 0.5610 - val_acc: 0.8579\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 9s - loss: 0.0374 - acc: 0.9846 - val_loss: 0.4422 - val_acc: 0.8703\n",
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 0.90037, saving model to best_m.h5\n",
      " - 28s - loss: 0.4022 - acc: 0.8032 - val_loss: 0.9004 - val_acc: 0.5461\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 9s - loss: 0.2948 - acc: 0.8693 - val_loss: 0.9869 - val_acc: 0.5461\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.90037 to 0.52412, saving model to best_m.h5\n",
      " - 10s - loss: 0.2583 - acc: 0.8826 - val_loss: 0.5241 - val_acc: 0.7880\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.52412 to 0.49745, saving model to best_m.h5\n",
      " - 9s - loss: 0.2206 - acc: 0.9015 - val_loss: 0.4975 - val_acc: 0.7506\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.49745 to 0.34516, saving model to best_m.h5\n",
      " - 9s - loss: 0.1994 - acc: 0.9119 - val_loss: 0.3452 - val_acc: 0.8130\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 9s - loss: 0.1665 - acc: 0.9298 - val_loss: 0.5013 - val_acc: 0.7930\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 10s - loss: 0.1484 - acc: 0.9393 - val_loss: 0.5694 - val_acc: 0.8329\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.34516 to 0.34119, saving model to best_m.h5\n",
      " - 10s - loss: 0.1173 - acc: 0.9512 - val_loss: 0.3412 - val_acc: 0.8529\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 9s - loss: 0.1083 - acc: 0.9611 - val_loss: 0.5305 - val_acc: 0.8479\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 9s - loss: 0.0883 - acc: 0.9661 - val_loss: 0.5211 - val_acc: 0.8329\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 9s - loss: 0.0755 - acc: 0.9713 - val_loss: 0.4300 - val_acc: 0.8304\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 10s - loss: 0.0534 - acc: 0.9807 - val_loss: 0.4391 - val_acc: 0.8803\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 10s - loss: 0.0493 - acc: 0.9821 - val_loss: 0.4693 - val_acc: 0.8379\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 9s - loss: 0.0475 - acc: 0.9848 - val_loss: 0.3952 - val_acc: 0.8603\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 9s - loss: 0.0499 - acc: 0.9800 - val_loss: 0.3798 - val_acc: 0.8778\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 9s - loss: 0.0311 - acc: 0.9886 - val_loss: 0.4791 - val_acc: 0.8753\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 9s - loss: 0.0321 - acc: 0.9886 - val_loss: 0.5996 - val_acc: 0.8454\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 9s - loss: 0.0278 - acc: 0.9904 - val_loss: 0.5802 - val_acc: 0.8379\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 9s - loss: 0.0266 - acc: 0.9911 - val_loss: 0.9082 - val_acc: 0.8304\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 9s - loss: 0.0368 - acc: 0.9900 - val_loss: 0.5383 - val_acc: 0.8279\n",
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.12632, saving model to best_m.h5\n",
      " - 33s - loss: 0.4382 - acc: 0.7882 - val_loss: 1.1263 - val_acc: 0.5611\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 9s - loss: 0.3185 - acc: 0.8531 - val_loss: 1.3308 - val_acc: 0.5611\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 1.12632 to 0.75399, saving model to best_m.h5\n",
      " - 10s - loss: 0.2815 - acc: 0.8782 - val_loss: 0.7540 - val_acc: 0.5611\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.75399 to 0.32776, saving model to best_m.h5\n",
      " - 10s - loss: 0.2427 - acc: 0.8919 - val_loss: 0.3278 - val_acc: 0.8579\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 10s - loss: 0.2205 - acc: 0.9059 - val_loss: 0.3417 - val_acc: 0.8554\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 9s - loss: 0.1869 - acc: 0.9187 - val_loss: 0.5380 - val_acc: 0.7830\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 9s - loss: 0.1640 - acc: 0.9356 - val_loss: 0.5050 - val_acc: 0.7905\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.32776 to 0.27250, saving model to best_m.h5\n",
      " - 10s - loss: 0.1262 - acc: 0.9485 - val_loss: 0.2725 - val_acc: 0.9002\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 9s - loss: 0.1202 - acc: 0.9530 - val_loss: 0.3828 - val_acc: 0.8803\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 9s - loss: 0.1061 - acc: 0.9607 - val_loss: 0.7589 - val_acc: 0.8304\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 9s - loss: 0.0662 - acc: 0.9744 - val_loss: 0.6877 - val_acc: 0.7905\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 9s - loss: 0.0616 - acc: 0.9767 - val_loss: 0.5139 - val_acc: 0.8329\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 9s - loss: 0.0579 - acc: 0.9790 - val_loss: 0.4146 - val_acc: 0.8778\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 10s - loss: 0.0559 - acc: 0.9765 - val_loss: 0.5201 - val_acc: 0.8180\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 9s - loss: 0.0610 - acc: 0.9769 - val_loss: 0.5664 - val_acc: 0.8080\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 10s - loss: 0.0328 - acc: 0.9879 - val_loss: 1.0466 - val_acc: 0.8180\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 9s - loss: 0.0361 - acc: 0.9886 - val_loss: 1.0089 - val_acc: 0.7731\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 9s - loss: 0.0436 - acc: 0.9861 - val_loss: 0.6371 - val_acc: 0.8603\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 9s - loss: 0.0412 - acc: 0.9861 - val_loss: 0.6168 - val_acc: 0.8055\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 9s - loss: 0.0487 - acc: 0.9825 - val_loss: 0.9719 - val_acc: 0.8429\n",
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.19509, saving model to best_m.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 38s - loss: 0.4073 - acc: 0.8026 - val_loss: 1.1951 - val_acc: 0.5062\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.19509 to 0.66883, saving model to best_m.h5\n",
      " - 9s - loss: 0.2950 - acc: 0.8587 - val_loss: 0.6688 - val_acc: 0.5087\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.66883 to 0.51426, saving model to best_m.h5\n",
      " - 10s - loss: 0.2474 - acc: 0.8867 - val_loss: 0.5143 - val_acc: 0.7456\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.51426 to 0.35914, saving model to best_m.h5\n",
      " - 10s - loss: 0.2300 - acc: 0.8971 - val_loss: 0.3591 - val_acc: 0.8304\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 9s - loss: 0.1935 - acc: 0.9194 - val_loss: 0.4309 - val_acc: 0.8429\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 9s - loss: 0.1880 - acc: 0.9196 - val_loss: 0.4407 - val_acc: 0.8055\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 10s - loss: 0.1543 - acc: 0.9296 - val_loss: 0.5825 - val_acc: 0.7980\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 10s - loss: 0.1357 - acc: 0.9429 - val_loss: 1.1320 - val_acc: 0.6135\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 9s - loss: 0.1309 - acc: 0.9460 - val_loss: 0.5776 - val_acc: 0.7880\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 9s - loss: 0.1026 - acc: 0.9601 - val_loss: 0.6317 - val_acc: 0.8204\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 10s - loss: 0.0871 - acc: 0.9657 - val_loss: 0.6469 - val_acc: 0.8379\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 9s - loss: 0.0734 - acc: 0.9749 - val_loss: 0.5204 - val_acc: 0.8379\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 10s - loss: 0.0576 - acc: 0.9782 - val_loss: 0.9628 - val_acc: 0.8130\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 9s - loss: 0.0578 - acc: 0.9800 - val_loss: 0.5476 - val_acc: 0.8529\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 10s - loss: 0.0528 - acc: 0.9807 - val_loss: 0.5848 - val_acc: 0.8354\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 9s - loss: 0.0531 - acc: 0.9815 - val_loss: 0.5986 - val_acc: 0.8304\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 10s - loss: 0.0351 - acc: 0.9871 - val_loss: 0.5889 - val_acc: 0.8579\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 9s - loss: 0.0469 - acc: 0.9855 - val_loss: 0.7451 - val_acc: 0.8479\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 9s - loss: 0.0286 - acc: 0.9911 - val_loss: 0.8545 - val_acc: 0.8155\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 10s - loss: 0.0366 - acc: 0.9890 - val_loss: 0.6327 - val_acc: 0.8204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kfold_train(fold_cnt=3,rnd=42):\n",
    "    train_pred, test_pred = np.zeros((1604,1)),np.zeros((8424,1))\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=True, random_state=2*rnd)\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        curr_x,curr_y = train_x[train_index],y[train_index]\n",
    "        curr_x,curr_y = rot_aut(curr_x,curr_y)\n",
    "        val_x,val_y = train_x[test_index],y[test_index]\n",
    "        \n",
    "        model = create_model()\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(0.0005), metrics=['accuracy'])\n",
    "        model_p = 'best_m.h5'\n",
    "        model_chk = ModelCheckpoint(filepath=model_p, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        model.fit(curr_x,curr_y,\n",
    "                  validation_data=(val_x,val_y),\n",
    "                  batch_size=32, epochs=20, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk]\n",
    "                 )\n",
    "        model = load_model(model_p)\n",
    "        \n",
    "        train_pred[test_index] = model.predict(val_x)\n",
    "        test_pred = test_pred + model.predict(test_x)/fold_cnt\n",
    "    return train_pred,test_pred\n",
    "\n",
    "train_pred,test_pred = kfold_train(fold_cnt=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.303490242014\n",
      "         id  is_iceberg\n",
      "0  5941774d    0.413197\n",
      "1  4023181e    0.433543\n",
      "2  b20200e4    0.179360\n",
      "3  e7f018bb    0.995098\n",
      "4  4371c8c3    0.333573\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../features/incept_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "\n",
    "# train feat loss\n",
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y,train_pred))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_df['id']\n",
    "submission['is_iceberg']=test_pred\n",
    "print(submission.head())\n",
    "submission.to_csv('../results/incept_1_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
