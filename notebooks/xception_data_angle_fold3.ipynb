{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "train_df = pd.read_json('../input/train.json')\n",
    "test_df = pd.read_json('../input/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75, 3) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "def std_img(x):\n",
    "    for i in range(3):\n",
    "        x[:, :, i] -= np.mean(x[:, :, i].flatten())\n",
    "        x[:, :, i] /= np.std(x[:, :, i].flatten()) + 1e-7\n",
    "    return x\n",
    "\n",
    "def get_image(df):\n",
    "    '''Create 3-channel 'images'. Return rescale-normalised images.'''\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Formulate the bands as 75x75 arrays\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        if row['inc_angle'] == 'na':\n",
    "            ang = -1\n",
    "        else:\n",
    "            ang = float(row['inc_angle'])\n",
    "        \n",
    "        band_3 = (band_1 - band_2)*ang/2\n",
    "\n",
    "        # Rescale\n",
    "        img = np.dstack([band_1,band_2,band_3])\n",
    "        img = std_img(img)\n",
    "\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "train_x = get_image(train_df)\n",
    "test_x = get_image(test_df)\n",
    "\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = train_df.is_iceberg.values\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 37, 37, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 37, 37, 32)   128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 37, 37, 32)   0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 35, 35, 64)   18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 35, 35, 64)   256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 35, 35, 64)   0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 35, 35, 128)  8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 35, 35, 128)  512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 35, 35, 128)  0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 35, 35, 128)  17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 35, 35, 128)  512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 18, 18, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 18, 18, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 18, 18, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 18, 18, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 18, 18, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 18, 18, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 18, 18, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 18, 18, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 18, 18, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 18, 18, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 9, 9, 256)    32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 9, 9, 256)    0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 9, 9, 256)    1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 9, 9, 256)    0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 9, 9, 256)    0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 9, 9, 728)    188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 9, 9, 728)    2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 9, 9, 728)    0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 9, 9, 728)    536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 9, 9, 728)    2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 5, 5, 728)    186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 5, 5, 728)    0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 5, 5, 728)    2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 5, 5, 728)    0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 5, 5, 728)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 5, 5, 728)    0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 5, 5, 728)    0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 5, 5, 728)    0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 5, 5, 728)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 5, 5, 728)    0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 5, 5, 728)    0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 5, 5, 728)    0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 5, 5, 728)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 5, 5, 728)    0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 5, 5, 728)    0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 5, 5, 728)    0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 5, 5, 728)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 5, 5, 728)    0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 5, 5, 728)    0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 5, 5, 728)    0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 5, 5, 728)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 5, 5, 728)    0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 5, 5, 728)    0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 5, 5, 728)    536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 5, 5, 728)    2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 5, 5, 728)    0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 5, 5, 728)    0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 5, 5, 728)    0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 5, 5, 728)    0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 5, 5, 728)    0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 5, 5, 728)    0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 5, 5, 728)    0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 5, 5, 728)    0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 5, 5, 728)    0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 5, 5, 728)    536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 5, 5, 728)    2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 5, 5, 728)    0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 5, 5, 728)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 5, 5, 728)    536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 5, 5, 728)    2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 5, 5, 728)    0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 5, 5, 1024)   752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 5, 5, 1024)   4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 1024)   745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 3, 3, 1024)   0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 3, 3, 1024)   4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 3, 3, 1024)   0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 3, 3, 1536)   1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 3, 3, 1536)   6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 3, 3, 1536)   0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 3, 3, 2048)   3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 3, 3, 2048)   8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 3, 3, 2048)   0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     global_max_pooling2d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,911,081\n",
      "Trainable params: 21,856,553\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "from keras.models import Model\n",
    "from keras.applications.xception import Xception\n",
    "def create_model():\n",
    "    base_model = Xception(include_top=False, weights=None, input_tensor=None, input_shape=(75,75,3), pooling='max')\n",
    "    x = Dense(512,activation='relu')(base_model.output)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(1,activation='sigmoid')(x)\n",
    "    return Model(inputs=base_model.input, outputs=x)\n",
    "print('model model')\n",
    "\n",
    "\n",
    "tmp_m = create_model()\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "Epoch 00001: val_loss improved from inf to 0.69331, saving model to best_m.h5\n",
      " - 27s - loss: 0.7634 - acc: 0.6471 - val_loss: 0.6933 - val_acc: 0.4505\n",
      "Epoch 2/80\n",
      "Epoch 00002: val_loss improved from 0.69331 to 0.69144, saving model to best_m.h5\n",
      " - 15s - loss: 0.5875 - acc: 0.7679 - val_loss: 0.6914 - val_acc: 0.5495\n",
      "Epoch 3/80\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 15s - loss: 0.6179 - acc: 0.7116 - val_loss: 0.6975 - val_acc: 0.4505\n",
      "Epoch 4/80\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 14s - loss: 0.4709 - acc: 0.7762 - val_loss: 0.6965 - val_acc: 0.4710\n",
      "Epoch 5/80\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 14s - loss: 0.4674 - acc: 0.8063 - val_loss: 1.1122 - val_acc: 0.4579\n",
      "Epoch 6/80\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 14s - loss: 0.4331 - acc: 0.8044 - val_loss: 0.9528 - val_acc: 0.5589\n",
      "Epoch 7/80\n",
      "Epoch 00007: val_loss improved from 0.69144 to 0.50724, saving model to best_m.h5\n",
      " - 15s - loss: 0.3748 - acc: 0.8595 - val_loss: 0.5072 - val_acc: 0.6748\n",
      "Epoch 8/80\n",
      "Epoch 00008: val_loss improved from 0.50724 to 0.33802, saving model to best_m.h5\n",
      " - 21s - loss: 0.3781 - acc: 0.8250 - val_loss: 0.3380 - val_acc: 0.8542\n",
      "Epoch 9/80\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 15s - loss: 0.5926 - acc: 0.7894 - val_loss: 1.0652 - val_acc: 0.5458\n",
      "Epoch 10/80\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 15s - loss: 0.5349 - acc: 0.7669 - val_loss: 1.1850 - val_acc: 0.5682\n",
      "Epoch 11/80\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 15s - loss: 0.4850 - acc: 0.8091 - val_loss: 0.3646 - val_acc: 0.8449\n",
      "Epoch 12/80\n",
      "Epoch 00012: val_loss improved from 0.33802 to 0.33117, saving model to best_m.h5\n",
      " - 15s - loss: 0.4196 - acc: 0.8474 - val_loss: 0.3312 - val_acc: 0.8598\n",
      "Epoch 13/80\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 14s - loss: 0.3737 - acc: 0.8287 - val_loss: 0.3471 - val_acc: 0.8131\n",
      "Epoch 14/80\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 14s - loss: 0.3165 - acc: 0.8595 - val_loss: 0.4044 - val_acc: 0.7925\n",
      "Epoch 15/80\n",
      "Epoch 00015: val_loss improved from 0.33117 to 0.24464, saving model to best_m.h5\n",
      " - 16s - loss: 0.3739 - acc: 0.8428 - val_loss: 0.2446 - val_acc: 0.8916\n",
      "Epoch 16/80\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 14s - loss: 0.3298 - acc: 0.8670 - val_loss: 0.2750 - val_acc: 0.8841\n",
      "Epoch 17/80\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 14s - loss: 0.3076 - acc: 0.8642 - val_loss: 0.3978 - val_acc: 0.7963\n",
      "Epoch 18/80\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 15s - loss: 0.3177 - acc: 0.8642 - val_loss: 0.2564 - val_acc: 0.8841\n",
      "Epoch 19/80\n",
      "Epoch 00019: val_loss improved from 0.24464 to 0.23985, saving model to best_m.h5\n",
      " - 15s - loss: 0.3729 - acc: 0.8550 - val_loss: 0.2399 - val_acc: 0.9196\n",
      "Epoch 20/80\n",
      "Epoch 00020: val_loss improved from 0.23985 to 0.23575, saving model to best_m.h5\n",
      " - 16s - loss: 0.2854 - acc: 0.8849 - val_loss: 0.2357 - val_acc: 0.9084\n",
      "Epoch 21/80\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 14s - loss: 0.2785 - acc: 0.8876 - val_loss: 0.2579 - val_acc: 0.8897\n",
      "Epoch 22/80\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 14s - loss: 0.2978 - acc: 0.8876 - val_loss: 0.3106 - val_acc: 0.8710\n",
      "Epoch 23/80\n",
      "Epoch 00023: val_loss improved from 0.23575 to 0.23355, saving model to best_m.h5\n",
      " - 15s - loss: 0.3116 - acc: 0.8633 - val_loss: 0.2336 - val_acc: 0.9047\n",
      "Epoch 24/80\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 14s - loss: 0.3213 - acc: 0.8793 - val_loss: 0.2726 - val_acc: 0.8804\n",
      "Epoch 25/80\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 14s - loss: 0.3201 - acc: 0.8726 - val_loss: 0.2800 - val_acc: 0.8710\n",
      "Epoch 26/80\n",
      "Epoch 00026: val_loss improved from 0.23355 to 0.22910, saving model to best_m.h5\n",
      " - 15s - loss: 0.3063 - acc: 0.8914 - val_loss: 0.2291 - val_acc: 0.9178\n",
      "Epoch 27/80\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 14s - loss: 0.2515 - acc: 0.8886 - val_loss: 0.2386 - val_acc: 0.9140\n",
      "Epoch 28/80\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 14s - loss: 0.3051 - acc: 0.8579 - val_loss: 0.2749 - val_acc: 0.8972\n",
      "Epoch 29/80\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 14s - loss: 0.2883 - acc: 0.8829 - val_loss: 0.4631 - val_acc: 0.8280\n",
      "Epoch 30/80\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 15s - loss: 0.2791 - acc: 0.8886 - val_loss: 0.2560 - val_acc: 0.8748\n",
      "Epoch 31/80\n",
      "Epoch 00031: val_loss improved from 0.22910 to 0.21553, saving model to best_m.h5\n",
      " - 15s - loss: 0.2795 - acc: 0.8701 - val_loss: 0.2155 - val_acc: 0.8935\n",
      "Epoch 32/80\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 14s - loss: 0.2403 - acc: 0.9037 - val_loss: 0.2322 - val_acc: 0.9047\n",
      "Epoch 33/80\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 14s - loss: 0.2206 - acc: 0.8951 - val_loss: 0.2315 - val_acc: 0.9290\n",
      "Epoch 34/80\n",
      "Epoch 00034: val_loss improved from 0.21553 to 0.21195, saving model to best_m.h5\n",
      " - 15s - loss: 0.2147 - acc: 0.9120 - val_loss: 0.2120 - val_acc: 0.9121\n",
      "Epoch 35/80\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 14s - loss: 0.2330 - acc: 0.8998 - val_loss: 0.2376 - val_acc: 0.9065\n",
      "Epoch 36/80\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 14s - loss: 0.2711 - acc: 0.8804 - val_loss: 0.2274 - val_acc: 0.9196\n",
      "Epoch 37/80\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 14s - loss: 0.2160 - acc: 0.9139 - val_loss: 0.2405 - val_acc: 0.9065\n",
      "Epoch 38/80\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 14s - loss: 0.2226 - acc: 0.9082 - val_loss: 0.2189 - val_acc: 0.9028\n",
      "Epoch 39/80\n",
      "Epoch 00039: val_loss improved from 0.21195 to 0.20757, saving model to best_m.h5\n",
      " - 21s - loss: 0.2437 - acc: 0.8999 - val_loss: 0.2076 - val_acc: 0.9140\n",
      "Epoch 40/80\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 14s - loss: 0.2249 - acc: 0.8998 - val_loss: 0.2212 - val_acc: 0.9178\n",
      "Epoch 41/80\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 14s - loss: 0.2094 - acc: 0.9026 - val_loss: 0.2132 - val_acc: 0.9065\n",
      "Epoch 42/80\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 14s - loss: 0.2244 - acc: 0.8971 - val_loss: 0.2244 - val_acc: 0.9103\n",
      "Epoch 43/80\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 14s - loss: 0.2252 - acc: 0.9073 - val_loss: 0.2116 - val_acc: 0.9159\n",
      "Epoch 44/80\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 14s - loss: 0.2007 - acc: 0.9233 - val_loss: 0.2583 - val_acc: 0.8991\n",
      "Epoch 45/80\n",
      "Epoch 00045: val_loss did not improve\n",
      " - 14s - loss: 0.2095 - acc: 0.8971 - val_loss: 0.2233 - val_acc: 0.8991\n",
      "Epoch 46/80\n",
      "Epoch 00046: val_loss did not improve\n",
      " - 14s - loss: 0.2140 - acc: 0.9110 - val_loss: 0.2174 - val_acc: 0.9121\n",
      "Epoch 47/80\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 14s - loss: 0.2275 - acc: 0.9102 - val_loss: 0.2135 - val_acc: 0.9047\n",
      "Epoch 48/80\n",
      "Epoch 00048: val_loss improved from 0.20757 to 0.20623, saving model to best_m.h5\n",
      " - 15s - loss: 0.2098 - acc: 0.9073 - val_loss: 0.2062 - val_acc: 0.9140\n",
      "Epoch 49/80\n",
      "Epoch 00049: val_loss did not improve\n",
      " - 14s - loss: 0.2373 - acc: 0.8869 - val_loss: 0.2423 - val_acc: 0.8879\n",
      "Epoch 50/80\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 14s - loss: 0.2422 - acc: 0.8962 - val_loss: 0.2109 - val_acc: 0.9103\n",
      "Epoch 51/80\n",
      "Epoch 00051: val_loss improved from 0.20623 to 0.19721, saving model to best_m.h5\n",
      " - 15s - loss: 0.2072 - acc: 0.9195 - val_loss: 0.1972 - val_acc: 0.9215\n",
      "Epoch 52/80\n",
      "Epoch 00052: val_loss did not improve\n",
      " - 14s - loss: 0.1923 - acc: 0.9167 - val_loss: 0.2139 - val_acc: 0.9028\n",
      "Epoch 53/80\n",
      "Epoch 00053: val_loss did not improve\n",
      " - 14s - loss: 0.1800 - acc: 0.9149 - val_loss: 0.2091 - val_acc: 0.9140\n",
      "Epoch 54/80\n",
      "Epoch 00054: val_loss did not improve\n",
      " - 14s - loss: 0.2144 - acc: 0.9139 - val_loss: 0.2046 - val_acc: 0.9252\n",
      "Epoch 55/80\n",
      "Epoch 00055: val_loss did not improve\n",
      " - 14s - loss: 0.1958 - acc: 0.9148 - val_loss: 0.2491 - val_acc: 0.9084\n",
      "Epoch 56/80\n",
      "Epoch 00056: val_loss did not improve\n",
      " - 14s - loss: 0.2196 - acc: 0.8999 - val_loss: 0.4838 - val_acc: 0.8280\n",
      "Epoch 57/80\n",
      "Epoch 00057: val_loss did not improve\n",
      " - 14s - loss: 0.1848 - acc: 0.9326 - val_loss: 0.2092 - val_acc: 0.9271\n",
      "Epoch 58/80\n",
      "Epoch 00058: val_loss did not improve\n",
      " - 14s - loss: 0.1864 - acc: 0.9196 - val_loss: 0.2027 - val_acc: 0.9234\n",
      "Epoch 59/80\n",
      "Epoch 00059: val_loss improved from 0.19721 to 0.19202, saving model to best_m.h5\n",
      " - 15s - loss: 0.1956 - acc: 0.9110 - val_loss: 0.1920 - val_acc: 0.9234\n",
      "Epoch 60/80\n",
      "Epoch 00060: val_loss did not improve\n",
      " - 14s - loss: 0.2016 - acc: 0.9195 - val_loss: 0.2241 - val_acc: 0.9028\n",
      "Epoch 61/80\n",
      "Epoch 00061: val_loss did not improve\n",
      " - 14s - loss: 0.1773 - acc: 0.9326 - val_loss: 0.2408 - val_acc: 0.9065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/80\n",
      "Epoch 00062: val_loss did not improve\n",
      " - 14s - loss: 0.2007 - acc: 0.9204 - val_loss: 0.2040 - val_acc: 0.9178\n",
      "Epoch 63/80\n",
      "Epoch 00063: val_loss did not improve\n",
      " - 14s - loss: 0.2244 - acc: 0.8972 - val_loss: 0.3060 - val_acc: 0.9103\n",
      "Epoch 64/80\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 14s - loss: 0.1995 - acc: 0.9157 - val_loss: 0.3831 - val_acc: 0.8953\n",
      "Epoch 65/80\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 14s - loss: 0.1849 - acc: 0.9270 - val_loss: 0.2308 - val_acc: 0.9121\n",
      "Epoch 66/80\n",
      "Epoch 00066: val_loss improved from 0.19202 to 0.18936, saving model to best_m.h5\n",
      " - 15s - loss: 0.1976 - acc: 0.9075 - val_loss: 0.1894 - val_acc: 0.9290\n",
      "Epoch 67/80\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 14s - loss: 0.2163 - acc: 0.9101 - val_loss: 0.2026 - val_acc: 0.9159\n",
      "Epoch 68/80\n",
      "Epoch 00068: val_loss did not improve\n",
      " - 14s - loss: 0.1700 - acc: 0.9232 - val_loss: 0.2045 - val_acc: 0.9196\n",
      "Epoch 69/80\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 14s - loss: 0.1838 - acc: 0.9401 - val_loss: 0.3041 - val_acc: 0.9121\n",
      "Epoch 70/80\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 14s - loss: 0.1621 - acc: 0.9326 - val_loss: 0.3545 - val_acc: 0.9140\n",
      "Epoch 71/80\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 14s - loss: 0.1778 - acc: 0.9316 - val_loss: 0.2900 - val_acc: 0.8579\n",
      "Epoch 72/80\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 14s - loss: 0.1599 - acc: 0.9316 - val_loss: 0.2179 - val_acc: 0.9178\n",
      "Epoch 73/80\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 14s - loss: 0.2150 - acc: 0.9028 - val_loss: 0.2154 - val_acc: 0.9140\n",
      "Epoch 74/80\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 15s - loss: 0.1670 - acc: 0.9345 - val_loss: 0.3095 - val_acc: 0.9103\n",
      "Epoch 75/80\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 14s - loss: 0.1819 - acc: 0.9280 - val_loss: 0.2081 - val_acc: 0.9121\n",
      "Epoch 76/80\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 14s - loss: 0.1877 - acc: 0.9140 - val_loss: 0.1993 - val_acc: 0.9159\n",
      "Epoch 77/80\n",
      "Epoch 00077: val_loss did not improve\n",
      " - 14s - loss: 0.1414 - acc: 0.9485 - val_loss: 0.2181 - val_acc: 0.9234\n",
      "Epoch 78/80\n",
      "Epoch 00078: val_loss did not improve\n",
      " - 14s - loss: 0.1884 - acc: 0.9243 - val_loss: 0.2588 - val_acc: 0.8972\n",
      "Epoch 79/80\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 14s - loss: 0.1748 - acc: 0.9344 - val_loss: 0.2096 - val_acc: 0.9215\n",
      "Epoch 80/80\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 14s - loss: 0.1552 - acc: 0.9373 - val_loss: 0.2955 - val_acc: 0.8785\n",
      "============================\n",
      "Epoch 1/80\n",
      "Epoch 00001: val_loss improved from inf to 0.69285, saving model to best_m.h5\n",
      " - 38s - loss: 0.6625 - acc: 0.6994 - val_loss: 0.6928 - val_acc: 0.5234\n",
      "Epoch 2/80\n",
      "Epoch 00002: val_loss improved from 0.69285 to 0.69277, saving model to best_m.h5\n",
      " - 15s - loss: 0.5945 - acc: 0.7538 - val_loss: 0.6928 - val_acc: 0.5664\n",
      "Epoch 3/80\n",
      "Epoch 00003: val_loss improved from 0.69277 to 0.68913, saving model to best_m.h5\n",
      " - 16s - loss: 0.4708 - acc: 0.7668 - val_loss: 0.6891 - val_acc: 0.5346\n",
      "Epoch 4/80\n",
      "Epoch 00004: val_loss improved from 0.68913 to 0.67583, saving model to best_m.h5\n",
      " - 16s - loss: 0.4294 - acc: 0.8174 - val_loss: 0.6758 - val_acc: 0.5869\n",
      "Epoch 5/80\n",
      "Epoch 00005: val_loss improved from 0.67583 to 0.59519, saving model to best_m.h5\n",
      " - 16s - loss: 0.5976 - acc: 0.7931 - val_loss: 0.5952 - val_acc: 0.6430\n",
      "Epoch 6/80\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 14s - loss: 0.5517 - acc: 0.7537 - val_loss: 7.6825 - val_acc: 0.5234\n",
      "Epoch 7/80\n",
      "Epoch 00007: val_loss improved from 0.59519 to 0.58399, saving model to best_m.h5\n",
      " - 15s - loss: 0.5053 - acc: 0.7631 - val_loss: 0.5840 - val_acc: 0.7364\n",
      "Epoch 8/80\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 14s - loss: 0.5804 - acc: 0.7219 - val_loss: 3.1034 - val_acc: 0.5570\n",
      "Epoch 9/80\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 14s - loss: 0.6128 - acc: 0.7698 - val_loss: 7.3722 - val_acc: 0.5234\n",
      "Epoch 10/80\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 14s - loss: 0.4467 - acc: 0.8230 - val_loss: 0.8997 - val_acc: 0.7738\n",
      "Epoch 11/80\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 14s - loss: 0.3252 - acc: 0.8596 - val_loss: 0.7906 - val_acc: 0.7215\n",
      "Epoch 12/80\n",
      "Epoch 00012: val_loss improved from 0.58399 to 0.30415, saving model to best_m.h5\n",
      " - 15s - loss: 0.4984 - acc: 0.8353 - val_loss: 0.3042 - val_acc: 0.8692\n",
      "Epoch 13/80\n",
      "Epoch 00013: val_loss improved from 0.30415 to 0.29575, saving model to best_m.h5\n",
      " - 15s - loss: 0.3134 - acc: 0.8736 - val_loss: 0.2958 - val_acc: 0.8766\n",
      "Epoch 14/80\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 14s - loss: 0.3375 - acc: 0.8520 - val_loss: 0.3827 - val_acc: 0.8131\n",
      "Epoch 15/80\n",
      "Epoch 00015: val_loss improved from 0.29575 to 0.27192, saving model to best_m.h5\n",
      " - 22s - loss: 0.2902 - acc: 0.8829 - val_loss: 0.2719 - val_acc: 0.8766\n",
      "Epoch 16/80\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 15s - loss: 0.3126 - acc: 0.8698 - val_loss: 0.5025 - val_acc: 0.7421\n",
      "Epoch 17/80\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 15s - loss: 0.2886 - acc: 0.8783 - val_loss: 0.2784 - val_acc: 0.8710\n",
      "Epoch 18/80\n",
      "Epoch 00018: val_loss improved from 0.27192 to 0.23390, saving model to best_m.h5\n",
      " - 16s - loss: 0.4222 - acc: 0.8503 - val_loss: 0.2339 - val_acc: 0.8916\n",
      "Epoch 19/80\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 15s - loss: 0.3446 - acc: 0.8539 - val_loss: 0.2961 - val_acc: 0.8430\n",
      "Epoch 20/80\n",
      "Epoch 00020: val_loss improved from 0.23390 to 0.21561, saving model to best_m.h5\n",
      " - 16s - loss: 0.3094 - acc: 0.8792 - val_loss: 0.2156 - val_acc: 0.8972\n",
      "Epoch 21/80\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 15s - loss: 0.3222 - acc: 0.8558 - val_loss: 0.2233 - val_acc: 0.9215\n",
      "Epoch 22/80\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 15s - loss: 0.2554 - acc: 0.8933 - val_loss: 0.2315 - val_acc: 0.9009\n",
      "Epoch 23/80\n",
      "Epoch 00023: val_loss improved from 0.21561 to 0.19883, saving model to best_m.h5\n",
      " - 16s - loss: 0.4531 - acc: 0.8821 - val_loss: 0.1988 - val_acc: 0.9234\n",
      "Epoch 24/80\n",
      "Epoch 00024: val_loss improved from 0.19883 to 0.19825, saving model to best_m.h5\n",
      " - 16s - loss: 0.2508 - acc: 0.8867 - val_loss: 0.1983 - val_acc: 0.9196\n",
      "Epoch 25/80\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 14s - loss: 0.4154 - acc: 0.8943 - val_loss: 0.3570 - val_acc: 0.8598\n",
      "Epoch 26/80\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 15s - loss: 0.2719 - acc: 0.9007 - val_loss: 0.2384 - val_acc: 0.8897\n",
      "Epoch 27/80\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 15s - loss: 0.2655 - acc: 0.9007 - val_loss: 7.6307 - val_acc: 0.5234\n",
      "Epoch 28/80\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 15s - loss: 0.3139 - acc: 0.8530 - val_loss: 0.3196 - val_acc: 0.8860\n",
      "Epoch 29/80\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 14s - loss: 0.3519 - acc: 0.8662 - val_loss: 0.5199 - val_acc: 0.7850\n",
      "Epoch 30/80\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 14s - loss: 0.2609 - acc: 0.8914 - val_loss: 0.3131 - val_acc: 0.8579\n",
      "Epoch 31/80\n",
      "Epoch 00031: val_loss improved from 0.19825 to 0.17236, saving model to best_m.h5\n",
      " - 15s - loss: 0.2255 - acc: 0.9101 - val_loss: 0.1724 - val_acc: 0.9234\n",
      "Epoch 32/80\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 15s - loss: 0.4544 - acc: 0.8710 - val_loss: 0.1916 - val_acc: 0.9215\n",
      "Epoch 33/80\n",
      "Epoch 00033: val_loss improved from 0.17236 to 0.16273, saving model to best_m.h5\n",
      " - 16s - loss: 0.2180 - acc: 0.9045 - val_loss: 0.1627 - val_acc: 0.9533\n",
      "Epoch 34/80\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 17s - loss: 0.2052 - acc: 0.9260 - val_loss: 0.1774 - val_acc: 0.9252\n",
      "Epoch 35/80\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 15s - loss: 0.2366 - acc: 0.9054 - val_loss: 0.1662 - val_acc: 0.9439\n",
      "Epoch 36/80\n",
      "Epoch 00036: val_loss did not improve\n",
      " - 15s - loss: 0.4048 - acc: 0.8352 - val_loss: 0.3440 - val_acc: 0.8299\n",
      "Epoch 37/80\n",
      "Epoch 00037: val_loss did not improve\n",
      " - 16s - loss: 0.3150 - acc: 0.8746 - val_loss: 0.1920 - val_acc: 0.9290\n",
      "Epoch 38/80\n",
      "Epoch 00038: val_loss did not improve\n",
      " - 15s - loss: 0.2854 - acc: 0.8869 - val_loss: 0.1945 - val_acc: 0.9178\n",
      "Epoch 39/80\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 14s - loss: 0.2738 - acc: 0.9046 - val_loss: 0.1691 - val_acc: 0.9327\n",
      "Epoch 40/80\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 15s - loss: 0.2192 - acc: 0.9167 - val_loss: 0.1930 - val_acc: 0.9252\n",
      "Epoch 41/80\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 15s - loss: 0.2630 - acc: 0.8906 - val_loss: 0.1645 - val_acc: 0.9402\n",
      "Epoch 42/80\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 14s - loss: 0.2394 - acc: 0.8896 - val_loss: 0.1748 - val_acc: 0.9346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/80\n",
      "Epoch 00043: val_loss improved from 0.16273 to 0.15914, saving model to best_m.h5\n",
      " - 15s - loss: 0.1868 - acc: 0.9167 - val_loss: 0.1591 - val_acc: 0.9402\n",
      "Epoch 44/80\n",
      "Epoch 00044: val_loss improved from 0.15914 to 0.15776, saving model to best_m.h5\n",
      " - 15s - loss: 0.2255 - acc: 0.9055 - val_loss: 0.1578 - val_acc: 0.9439\n",
      "Epoch 45/80\n",
      "Epoch 00045: val_loss improved from 0.15776 to 0.15203, saving model to best_m.h5\n",
      " - 15s - loss: 0.1883 - acc: 0.9232 - val_loss: 0.1520 - val_acc: 0.9439\n",
      "Epoch 46/80\n",
      "Epoch 00046: val_loss did not improve\n",
      " - 14s - loss: 0.2029 - acc: 0.9195 - val_loss: 0.1567 - val_acc: 0.9327\n",
      "Epoch 47/80\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 14s - loss: 0.2022 - acc: 0.9167 - val_loss: 0.1627 - val_acc: 0.9346\n",
      "Epoch 48/80\n",
      "Epoch 00048: val_loss did not improve\n",
      " - 14s - loss: 0.2032 - acc: 0.9279 - val_loss: 0.1570 - val_acc: 0.9402\n",
      "Epoch 49/80\n",
      "Epoch 00049: val_loss did not improve\n",
      " - 14s - loss: 0.1918 - acc: 0.9241 - val_loss: 0.1758 - val_acc: 0.9178\n",
      "Epoch 50/80\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 14s - loss: 0.2554 - acc: 0.8999 - val_loss: 0.1621 - val_acc: 0.9383\n",
      "Epoch 51/80\n",
      "Epoch 00051: val_loss did not improve\n",
      " - 14s - loss: 0.1709 - acc: 0.9232 - val_loss: 0.1738 - val_acc: 0.9159\n",
      "Epoch 52/80\n",
      "Epoch 00052: val_loss did not improve\n",
      " - 14s - loss: 0.1770 - acc: 0.9241 - val_loss: 0.1571 - val_acc: 0.9252\n",
      "Epoch 53/80\n",
      "Epoch 00053: val_loss did not improve\n",
      " - 14s - loss: 0.1995 - acc: 0.9213 - val_loss: 0.1591 - val_acc: 0.9421\n",
      "Epoch 54/80\n",
      "Epoch 00054: val_loss did not improve\n",
      " - 14s - loss: 0.2386 - acc: 0.8990 - val_loss: 0.1752 - val_acc: 0.9234\n",
      "Epoch 55/80\n",
      "Epoch 00055: val_loss did not improve\n",
      " - 14s - loss: 0.2072 - acc: 0.9102 - val_loss: 0.1616 - val_acc: 0.9308\n",
      "Epoch 56/80\n",
      "Epoch 00056: val_loss did not improve\n",
      " - 14s - loss: 0.2508 - acc: 0.9205 - val_loss: 0.1553 - val_acc: 0.9383\n",
      "Epoch 57/80\n",
      "Epoch 00057: val_loss did not improve\n",
      " - 14s - loss: 0.2023 - acc: 0.9186 - val_loss: 0.1614 - val_acc: 0.9327\n",
      "Epoch 58/80\n",
      "Epoch 00058: val_loss did not improve\n",
      " - 14s - loss: 0.1987 - acc: 0.9166 - val_loss: 0.1530 - val_acc: 0.9271\n",
      "Epoch 59/80\n",
      "Epoch 00059: val_loss did not improve\n",
      " - 14s - loss: 0.1851 - acc: 0.9260 - val_loss: 0.1815 - val_acc: 0.9196\n",
      "Epoch 60/80\n",
      "Epoch 00060: val_loss did not improve\n",
      " - 14s - loss: 0.2789 - acc: 0.9083 - val_loss: 0.1679 - val_acc: 0.9234\n",
      "Epoch 61/80\n",
      "Epoch 00061: val_loss did not improve\n",
      " - 14s - loss: 0.2421 - acc: 0.9102 - val_loss: 0.1575 - val_acc: 0.9308\n",
      "Epoch 62/80\n",
      "Epoch 00062: val_loss did not improve\n",
      " - 14s - loss: 0.2100 - acc: 0.9205 - val_loss: 0.1871 - val_acc: 0.9252\n",
      "Epoch 63/80\n",
      "Epoch 00063: val_loss did not improve\n",
      " - 14s - loss: 0.1786 - acc: 0.9335 - val_loss: 0.1608 - val_acc: 0.9252\n",
      "Epoch 64/80\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 14s - loss: 0.2549 - acc: 0.9085 - val_loss: 0.1649 - val_acc: 0.9290\n",
      "Epoch 65/80\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 14s - loss: 0.2005 - acc: 0.9213 - val_loss: 0.2035 - val_acc: 0.9178\n",
      "Epoch 66/80\n",
      "Epoch 00066: val_loss did not improve\n",
      " - 14s - loss: 0.2417 - acc: 0.9093 - val_loss: 0.1931 - val_acc: 0.9159\n",
      "Epoch 67/80\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 14s - loss: 0.1766 - acc: 0.9307 - val_loss: 0.1684 - val_acc: 0.9271\n",
      "Epoch 68/80\n",
      "Epoch 00068: val_loss did not improve\n",
      " - 14s - loss: 0.2210 - acc: 0.9186 - val_loss: 0.1743 - val_acc: 0.9290\n",
      "Epoch 69/80\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 14s - loss: 0.1840 - acc: 0.9335 - val_loss: 0.1602 - val_acc: 0.9271\n",
      "Epoch 70/80\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 14s - loss: 0.1855 - acc: 0.9252 - val_loss: 0.1786 - val_acc: 0.9196\n",
      "Epoch 71/80\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 14s - loss: 0.1947 - acc: 0.9195 - val_loss: 0.1719 - val_acc: 0.9327\n",
      "Epoch 72/80\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 14s - loss: 0.1960 - acc: 0.9197 - val_loss: 0.1744 - val_acc: 0.9234\n",
      "Epoch 73/80\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 14s - loss: 0.1765 - acc: 0.9232 - val_loss: 0.1849 - val_acc: 0.9271\n",
      "Epoch 74/80\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 14s - loss: 0.1748 - acc: 0.9241 - val_loss: 0.1853 - val_acc: 0.9215\n",
      "Epoch 75/80\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 14s - loss: 0.2332 - acc: 0.9205 - val_loss: 0.1984 - val_acc: 0.9159\n",
      "Epoch 76/80\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 14s - loss: 0.1475 - acc: 0.9448 - val_loss: 0.1730 - val_acc: 0.9308\n",
      "Epoch 77/80\n",
      "Epoch 00077: val_loss did not improve\n",
      " - 14s - loss: 0.2089 - acc: 0.9065 - val_loss: 0.1613 - val_acc: 0.9308\n",
      "Epoch 78/80\n",
      "Epoch 00078: val_loss did not improve\n",
      " - 14s - loss: 0.1784 - acc: 0.9363 - val_loss: 0.1775 - val_acc: 0.9271\n",
      "Epoch 79/80\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 14s - loss: 0.2035 - acc: 0.9270 - val_loss: 0.1728 - val_acc: 0.9290\n",
      "Epoch 80/80\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 14s - loss: 0.2139 - acc: 0.9065 - val_loss: 0.1969 - val_acc: 0.9140\n",
      "============================\n",
      "Epoch 1/80\n",
      "Epoch 00001: val_loss improved from inf to 0.69345, saving model to best_m.h5\n",
      " - 50s - loss: 0.8455 - acc: 0.6340 - val_loss: 0.6934 - val_acc: 0.4813\n",
      "Epoch 2/80\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 14s - loss: 0.5827 - acc: 0.7378 - val_loss: 0.6950 - val_acc: 0.4813\n",
      "Epoch 3/80\n",
      "Epoch 00003: val_loss improved from 0.69345 to 0.69296, saving model to best_m.h5\n",
      " - 15s - loss: 0.4614 - acc: 0.7809 - val_loss: 0.6930 - val_acc: 0.4944\n",
      "Epoch 4/80\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 14s - loss: 0.4861 - acc: 0.7977 - val_loss: 0.6943 - val_acc: 0.4831\n",
      "Epoch 5/80\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 14s - loss: 0.4524 - acc: 0.8240 - val_loss: 0.7016 - val_acc: 0.4813\n",
      "Epoch 6/80\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 14s - loss: 0.6335 - acc: 0.6752 - val_loss: 2.5267 - val_acc: 0.5187\n",
      "Epoch 7/80\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 14s - loss: 0.6880 - acc: 0.5646 - val_loss: 0.7028 - val_acc: 0.4888\n",
      "Epoch 8/80\n",
      "Epoch 00008: val_loss improved from 0.69296 to 0.62347, saving model to best_m.h5\n",
      " - 15s - loss: 0.6743 - acc: 0.6207 - val_loss: 0.6235 - val_acc: 0.6948\n",
      "Epoch 9/80\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 14s - loss: 0.5612 - acc: 0.7285 - val_loss: 7.6646 - val_acc: 0.5187\n",
      "Epoch 10/80\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 14s - loss: 0.4807 - acc: 0.7678 - val_loss: 1.7587 - val_acc: 0.6948\n",
      "Epoch 11/80\n",
      "Epoch 00011: val_loss improved from 0.62347 to 0.39607, saving model to best_m.h5\n",
      " - 15s - loss: 0.4136 - acc: 0.8184 - val_loss: 0.3961 - val_acc: 0.8221\n",
      "Epoch 12/80\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 14s - loss: 0.3866 - acc: 0.8315 - val_loss: 0.4519 - val_acc: 0.8184\n",
      "Epoch 13/80\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 14s - loss: 0.4120 - acc: 0.8211 - val_loss: 0.5860 - val_acc: 0.7416\n",
      "Epoch 14/80\n",
      "Epoch 00014: val_loss improved from 0.39607 to 0.37391, saving model to best_m.h5\n",
      " - 15s - loss: 0.3805 - acc: 0.8417 - val_loss: 0.3739 - val_acc: 0.8408\n",
      "Epoch 15/80\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 14s - loss: 0.3781 - acc: 0.8484 - val_loss: 0.4165 - val_acc: 0.8202\n",
      "Epoch 16/80\n",
      "Epoch 00016: val_loss improved from 0.37391 to 0.27597, saving model to best_m.h5\n",
      " - 15s - loss: 0.3735 - acc: 0.8314 - val_loss: 0.2760 - val_acc: 0.8895\n",
      "Epoch 17/80\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 14s - loss: 0.3535 - acc: 0.8512 - val_loss: 0.4850 - val_acc: 0.8034\n",
      "Epoch 18/80\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 14s - loss: 0.3289 - acc: 0.8689 - val_loss: 0.4225 - val_acc: 0.7921\n",
      "Epoch 19/80\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 14s - loss: 0.3437 - acc: 0.8586 - val_loss: 0.2935 - val_acc: 0.8745\n",
      "Epoch 20/80\n",
      "Epoch 00020: val_loss improved from 0.27597 to 0.27419, saving model to best_m.h5\n",
      " - 15s - loss: 0.3253 - acc: 0.8596 - val_loss: 0.2742 - val_acc: 0.8783\n",
      "Epoch 21/80\n",
      "Epoch 00021: val_loss did not improve\n",
      " - 14s - loss: 0.3752 - acc: 0.8586 - val_loss: 0.2912 - val_acc: 0.8914\n",
      "Epoch 22/80\n",
      "Epoch 00022: val_loss did not improve\n",
      " - 14s - loss: 0.3528 - acc: 0.8446 - val_loss: 0.2943 - val_acc: 0.8801\n",
      "Epoch 23/80\n",
      "Epoch 00023: val_loss improved from 0.27419 to 0.25516, saving model to best_m.h5\n",
      " - 15s - loss: 0.3378 - acc: 0.8539 - val_loss: 0.2552 - val_acc: 0.8989\n",
      "Epoch 24/80\n",
      "Epoch 00024: val_loss did not improve\n",
      " - 14s - loss: 0.3173 - acc: 0.8783 - val_loss: 0.2590 - val_acc: 0.8858\n",
      "Epoch 25/80\n",
      "Epoch 00025: val_loss did not improve\n",
      " - 14s - loss: 0.3494 - acc: 0.8614 - val_loss: 0.3920 - val_acc: 0.8708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/80\n",
      "Epoch 00026: val_loss did not improve\n",
      " - 14s - loss: 0.3055 - acc: 0.8745 - val_loss: 0.3356 - val_acc: 0.8596\n",
      "Epoch 27/80\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 14s - loss: 0.3674 - acc: 0.8399 - val_loss: 0.2643 - val_acc: 0.8914\n",
      "Epoch 28/80\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 14s - loss: 0.2810 - acc: 0.8764 - val_loss: 0.3188 - val_acc: 0.8614\n",
      "Epoch 29/80\n",
      "Epoch 00029: val_loss did not improve\n",
      " - 14s - loss: 0.2917 - acc: 0.8708 - val_loss: 0.4708 - val_acc: 0.8127\n",
      "Epoch 30/80\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 14s - loss: 0.2987 - acc: 0.8811 - val_loss: 0.4745 - val_acc: 0.8502\n",
      "Epoch 31/80\n",
      "Epoch 00031: val_loss improved from 0.25516 to 0.24808, saving model to best_m.h5\n",
      " - 15s - loss: 0.2832 - acc: 0.8745 - val_loss: 0.2481 - val_acc: 0.8876\n",
      "Epoch 32/80\n",
      "Epoch 00032: val_loss did not improve\n",
      " - 14s - loss: 0.2562 - acc: 0.8839 - val_loss: 0.2560 - val_acc: 0.8914\n",
      "Epoch 33/80\n",
      "Epoch 00033: val_loss did not improve\n",
      " - 14s - loss: 0.2796 - acc: 0.8914 - val_loss: 0.2555 - val_acc: 0.8876\n",
      "Epoch 34/80\n",
      "Epoch 00034: val_loss did not improve\n",
      " - 14s - loss: 0.2560 - acc: 0.9045 - val_loss: 0.2664 - val_acc: 0.8933\n",
      "Epoch 35/80\n",
      "Epoch 00035: val_loss did not improve\n",
      " - 14s - loss: 0.2718 - acc: 0.8877 - val_loss: 0.2486 - val_acc: 0.9007\n",
      "Epoch 36/80\n",
      "Epoch 00036: val_loss improved from 0.24808 to 0.24782, saving model to best_m.h5\n",
      " - 15s - loss: 0.2574 - acc: 0.8914 - val_loss: 0.2478 - val_acc: 0.8895\n",
      "Epoch 37/80\n",
      "Epoch 00037: val_loss improved from 0.24782 to 0.24173, saving model to best_m.h5\n",
      " - 15s - loss: 0.2751 - acc: 0.8904 - val_loss: 0.2417 - val_acc: 0.8858\n",
      "Epoch 38/80\n",
      "Epoch 00038: val_loss improved from 0.24173 to 0.23765, saving model to best_m.h5\n",
      " - 15s - loss: 0.2854 - acc: 0.8830 - val_loss: 0.2377 - val_acc: 0.9026\n",
      "Epoch 39/80\n",
      "Epoch 00039: val_loss did not improve\n",
      " - 14s - loss: 0.2369 - acc: 0.9073 - val_loss: 0.2466 - val_acc: 0.8858\n",
      "Epoch 40/80\n",
      "Epoch 00040: val_loss did not improve\n",
      " - 14s - loss: 0.2788 - acc: 0.8877 - val_loss: 0.2491 - val_acc: 0.8951\n",
      "Epoch 41/80\n",
      "Epoch 00041: val_loss did not improve\n",
      " - 14s - loss: 0.2328 - acc: 0.8979 - val_loss: 0.2740 - val_acc: 0.8914\n",
      "Epoch 42/80\n",
      "Epoch 00042: val_loss did not improve\n",
      " - 14s - loss: 0.2577 - acc: 0.8932 - val_loss: 0.2419 - val_acc: 0.8970\n",
      "Epoch 43/80\n",
      "Epoch 00043: val_loss did not improve\n",
      " - 14s - loss: 0.2413 - acc: 0.8904 - val_loss: 0.2454 - val_acc: 0.8951\n",
      "Epoch 44/80\n",
      "Epoch 00044: val_loss did not improve\n",
      " - 14s - loss: 0.2541 - acc: 0.8942 - val_loss: 0.2582 - val_acc: 0.9007\n",
      "Epoch 45/80\n",
      "Epoch 00045: val_loss did not improve\n",
      " - 14s - loss: 0.2838 - acc: 0.8915 - val_loss: 0.2536 - val_acc: 0.8989\n",
      "Epoch 46/80\n",
      "Epoch 00046: val_loss improved from 0.23765 to 0.23286, saving model to best_m.h5\n",
      " - 15s - loss: 0.2403 - acc: 0.9036 - val_loss: 0.2329 - val_acc: 0.9007\n",
      "Epoch 47/80\n",
      "Epoch 00047: val_loss did not improve\n",
      " - 14s - loss: 0.2394 - acc: 0.9073 - val_loss: 0.2451 - val_acc: 0.9007\n",
      "Epoch 48/80\n",
      "Epoch 00048: val_loss improved from 0.23286 to 0.22995, saving model to best_m.h5\n",
      " - 15s - loss: 0.2132 - acc: 0.9036 - val_loss: 0.2299 - val_acc: 0.9064\n",
      "Epoch 49/80\n",
      "Epoch 00049: val_loss did not improve\n",
      " - 14s - loss: 0.2675 - acc: 0.8970 - val_loss: 0.2534 - val_acc: 0.8895\n",
      "Epoch 50/80\n",
      "Epoch 00050: val_loss did not improve\n",
      " - 14s - loss: 0.2740 - acc: 0.8867 - val_loss: 0.2975 - val_acc: 0.8839\n",
      "Epoch 51/80\n",
      "Epoch 00051: val_loss improved from 0.22995 to 0.22378, saving model to best_m.h5\n",
      " - 15s - loss: 0.2352 - acc: 0.9045 - val_loss: 0.2238 - val_acc: 0.9045\n",
      "Epoch 52/80\n",
      "Epoch 00052: val_loss did not improve\n",
      " - 14s - loss: 0.2416 - acc: 0.8952 - val_loss: 0.2510 - val_acc: 0.8839\n",
      "Epoch 53/80\n",
      "Epoch 00053: val_loss did not improve\n",
      " - 14s - loss: 0.2430 - acc: 0.8914 - val_loss: 0.2315 - val_acc: 0.9045\n",
      "Epoch 54/80\n",
      "Epoch 00054: val_loss did not improve\n",
      " - 15s - loss: 0.2456 - acc: 0.9017 - val_loss: 0.2243 - val_acc: 0.9082\n",
      "Epoch 55/80\n",
      "Epoch 00055: val_loss improved from 0.22378 to 0.21431, saving model to best_m.h5\n",
      " - 15s - loss: 0.1978 - acc: 0.9148 - val_loss: 0.2143 - val_acc: 0.9045\n",
      "Epoch 56/80\n",
      "Epoch 00056: val_loss did not improve\n",
      " - 14s - loss: 0.2146 - acc: 0.9120 - val_loss: 0.2151 - val_acc: 0.9007\n",
      "Epoch 57/80\n",
      "Epoch 00057: val_loss did not improve\n",
      " - 14s - loss: 0.2202 - acc: 0.9204 - val_loss: 0.2306 - val_acc: 0.9007\n",
      "Epoch 58/80\n",
      "Epoch 00058: val_loss did not improve\n",
      " - 14s - loss: 0.2607 - acc: 0.8905 - val_loss: 0.2215 - val_acc: 0.8970\n",
      "Epoch 59/80\n",
      "Epoch 00059: val_loss improved from 0.21431 to 0.21268, saving model to best_m.h5\n",
      " - 15s - loss: 0.2206 - acc: 0.9054 - val_loss: 0.2127 - val_acc: 0.9082\n",
      "Epoch 60/80\n",
      "Epoch 00060: val_loss did not improve\n",
      " - 14s - loss: 0.2566 - acc: 0.8895 - val_loss: 0.2365 - val_acc: 0.9101\n",
      "Epoch 61/80\n",
      "Epoch 00061: val_loss did not improve\n",
      " - 14s - loss: 0.2169 - acc: 0.9166 - val_loss: 0.2354 - val_acc: 0.8914\n",
      "Epoch 62/80\n",
      "Epoch 00062: val_loss did not improve\n",
      " - 14s - loss: 0.2412 - acc: 0.9045 - val_loss: 0.2287 - val_acc: 0.8989\n",
      "Epoch 63/80\n",
      "Epoch 00063: val_loss did not improve\n",
      " - 14s - loss: 0.2773 - acc: 0.8812 - val_loss: 0.4121 - val_acc: 0.7734\n",
      "Epoch 64/80\n",
      "Epoch 00064: val_loss did not improve\n",
      " - 14s - loss: 0.2370 - acc: 0.8979 - val_loss: 0.2182 - val_acc: 0.9082\n",
      "Epoch 65/80\n",
      "Epoch 00065: val_loss did not improve\n",
      " - 14s - loss: 0.2249 - acc: 0.8999 - val_loss: 0.2198 - val_acc: 0.9026\n",
      "Epoch 66/80\n",
      "Epoch 00066: val_loss did not improve\n",
      " - 14s - loss: 0.2377 - acc: 0.9111 - val_loss: 0.2196 - val_acc: 0.9045\n",
      "Epoch 67/80\n",
      "Epoch 00067: val_loss did not improve\n",
      " - 14s - loss: 0.2456 - acc: 0.8915 - val_loss: 0.2202 - val_acc: 0.9045\n",
      "Epoch 68/80\n",
      "Epoch 00068: val_loss improved from 0.21268 to 0.21252, saving model to best_m.h5\n",
      " - 15s - loss: 0.2175 - acc: 0.9035 - val_loss: 0.2125 - val_acc: 0.9082\n",
      "Epoch 69/80\n",
      "Epoch 00069: val_loss did not improve\n",
      " - 14s - loss: 0.1984 - acc: 0.9204 - val_loss: 0.2319 - val_acc: 0.9026\n",
      "Epoch 70/80\n",
      "Epoch 00070: val_loss did not improve\n",
      " - 14s - loss: 0.2171 - acc: 0.9166 - val_loss: 0.2235 - val_acc: 0.9026\n",
      "Epoch 71/80\n",
      "Epoch 00071: val_loss did not improve\n",
      " - 14s - loss: 0.2264 - acc: 0.9082 - val_loss: 0.2205 - val_acc: 0.9026\n",
      "Epoch 72/80\n",
      "Epoch 00072: val_loss did not improve\n",
      " - 14s - loss: 0.2308 - acc: 0.9017 - val_loss: 0.2515 - val_acc: 0.9007\n",
      "Epoch 73/80\n",
      "Epoch 00073: val_loss did not improve\n",
      " - 14s - loss: 0.1917 - acc: 0.9176 - val_loss: 0.2338 - val_acc: 0.9026\n",
      "Epoch 74/80\n",
      "Epoch 00074: val_loss did not improve\n",
      " - 14s - loss: 0.2508 - acc: 0.9073 - val_loss: 0.2231 - val_acc: 0.9082\n",
      "Epoch 75/80\n",
      "Epoch 00075: val_loss did not improve\n",
      " - 14s - loss: 0.2331 - acc: 0.9045 - val_loss: 0.2134 - val_acc: 0.9064\n",
      "Epoch 76/80\n",
      "Epoch 00076: val_loss did not improve\n",
      " - 14s - loss: 0.2257 - acc: 0.9187 - val_loss: 0.2573 - val_acc: 0.9064\n",
      "Epoch 77/80\n",
      "Epoch 00077: val_loss improved from 0.21252 to 0.21204, saving model to best_m.h5\n",
      " - 15s - loss: 0.1984 - acc: 0.9176 - val_loss: 0.2120 - val_acc: 0.9026\n",
      "Epoch 78/80\n",
      "Epoch 00078: val_loss did not improve\n",
      " - 14s - loss: 0.1998 - acc: 0.9241 - val_loss: 0.2797 - val_acc: 0.9007\n",
      "Epoch 79/80\n",
      "Epoch 00079: val_loss did not improve\n",
      " - 14s - loss: 0.1886 - acc: 0.9185 - val_loss: 0.2251 - val_acc: 0.9101\n",
      "Epoch 80/80\n",
      "Epoch 00080: val_loss did not improve\n",
      " - 14s - loss: 0.1968 - acc: 0.9204 - val_loss: 0.2395 - val_acc: 0.8989\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import gc\n",
    "\n",
    "def lr_f(epoch):\n",
    "    if epoch<10:\n",
    "        return 0.0008\n",
    "    elif epoch<30:\n",
    "        return 0.0004\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "def kfold_train(fold_cnt=3,rnd=99):\n",
    "    train_pred, test_pred = np.zeros((1604,1)),np.zeros((8424,1))\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=True, random_state=2*rnd)\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        curr_x,curr_y = train_x[train_index],y[train_index]\n",
    "        val_x,val_y = train_x[test_index],y[test_index]\n",
    "        datagen = ImageDataGenerator(\n",
    "            width_shift_range=0.05,\n",
    "            height_shift_range=0.05,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        bat_size = 12\n",
    "        steps_train = len(curr_y)//bat_size\n",
    "        \n",
    "        \n",
    "        model = create_model()\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(0.0005), metrics=['accuracy'])\n",
    "        model_p = 'best_m.h5'\n",
    "        model_chk = ModelCheckpoint(filepath=model_p, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        lr_s = LearningRateScheduler(lr_f)\n",
    "        model.fit_generator(datagen.flow(curr_x, curr_y, batch_size=bat_size),\n",
    "                  validation_data=(val_x,val_y),\n",
    "                  steps_per_epoch = steps_train,\n",
    "                  epochs=80, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk,lr_s]\n",
    "                 )\n",
    "        \n",
    "        \n",
    "        model = load_model(model_p)\n",
    "        train_pred[test_index] = model.predict(val_x)\n",
    "        test_pred = test_pred + model.predict(test_x)/fold_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('============================')\n",
    "    return train_pred,test_pred\n",
    "\n",
    "train_pred,test_pred = kfold_train(fold_cnt=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.184460799265\n",
      "         id  is_iceberg\n",
      "0  5941774d    0.020408\n",
      "1  4023181e    0.965778\n",
      "2  b20200e4    0.358534\n",
      "3  e7f018bb    0.997419\n",
      "4  4371c8c3    0.074239\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../2nd_features/xception_aug1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "\n",
    "# train feat loss\n",
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y,train_pred))\n",
    "    \n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_df['id']\n",
    "submission['is_iceberg']=test_pred\n",
    "print(submission.head())\n",
    "submission.to_csv('../2nd_features/xception_aug1_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
