{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, Activation,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "train_df = pd.read_json('../input/train.json')\n",
    "test_df = pd.read_json('../input/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 75, 75, 3) (8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_image(df):\n",
    "    '''Create 3-channel 'images'. Return rescale-normalised images.'''\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        # Formulate the bands as 75x75 arrays\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2\n",
    "\n",
    "        # Rescale\n",
    "        r = (band_1 - band_1.min()) / (band_1.max() - band_1.min())\n",
    "        g = (band_2 - band_2.min()) / (band_2.max() - band_2.min())\n",
    "        b = (band_3 - band_3.min()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        rgb = np.dstack((r, g, b))\n",
    "        images.append(rgb)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "train_x = get_image(train_df)\n",
    "test_x = get_image(test_df)\n",
    "\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "y = train_df.is_iceberg.values\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rot_aut(Xtr,ytr):\n",
    "    # aug on train\n",
    "    data_cnt = len(ytr)\n",
    "    print(data_cnt)\n",
    "    aug_X = []\n",
    "    aug_y = []\n",
    "\n",
    "    for i in range(data_cnt):\n",
    "        img = Xtr[i]\n",
    "        tmp_y = ytr[i]\n",
    "\n",
    "        # org img\n",
    "        aug_X.append(img)\n",
    "        aug_y.append(tmp_y)\n",
    "\n",
    "        # flip\n",
    "        tmp_img = np.fliplr(img)\n",
    "        aug_X.append(tmp_img)\n",
    "        aug_y.append(tmp_y)\n",
    "\n",
    "        tmp_img = np.flipud(img)\n",
    "        aug_X.append(tmp_img)\n",
    "        aug_y.append(tmp_y)\n",
    "\n",
    "        tmp_img = np.rot90(img)\n",
    "        aug_X.append(tmp_img)\n",
    "        aug_y.append(tmp_y)\n",
    "    return np.array(aug_X),np.array(aug_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 75, 75, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 38, 38, 64)   9472        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 38, 38, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_243 (Activation)     (None, 38, 38, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 18, 18, 64)   0           activation_243[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 18, 18, 64)   4160        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 18, 18, 64)   256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_244 (Activation)     (None, 18, 18, 64)   0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 18, 18, 64)   36928       activation_244[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 18, 18, 64)   256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_245 (Activation)     (None, 18, 18, 64)   0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 18, 18, 96)   6240        activation_245[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 18, 18, 96)   6240        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 18, 18, 96)   384         res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 18, 18, 96)   384         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 18, 18, 96)   0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_246 (Activation)     (None, 18, 18, 96)   0           add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 18, 18, 64)   6208        activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 18, 18, 64)   256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_247 (Activation)     (None, 18, 18, 64)   0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 18, 18, 64)   36928       activation_247[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 18, 18, 64)   256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_248 (Activation)     (None, 18, 18, 64)   0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 18, 18, 96)   6240        activation_248[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 18, 18, 96)   384         res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 18, 18, 96)   0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_246[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_249 (Activation)     (None, 18, 18, 96)   0           add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 18, 18, 64)   6208        activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 18, 18, 64)   256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_250 (Activation)     (None, 18, 18, 64)   0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 18, 18, 64)   36928       activation_250[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 18, 18, 64)   256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_251 (Activation)     (None, 18, 18, 64)   0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 18, 18, 96)   6240        activation_251[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 18, 18, 96)   384         res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 18, 18, 96)   0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_249[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_252 (Activation)     (None, 18, 18, 96)   0           add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 9, 9, 96)     9312        activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 9, 9, 96)     384         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_253 (Activation)     (None, 9, 9, 96)     0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 9, 9, 96)     83040       activation_253[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 9, 9, 96)     384         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_254 (Activation)     (None, 9, 9, 96)     0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 9, 9, 128)    12416       activation_254[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 9, 9, 128)    12416       activation_252[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 9, 9, 128)    512         res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 9, 9, 128)    512         res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 9, 9, 128)    0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_255 (Activation)     (None, 9, 9, 128)    0           add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 9, 9, 96)     12384       activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 9, 9, 96)     384         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_256 (Activation)     (None, 9, 9, 96)     0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 9, 9, 96)     83040       activation_256[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 9, 9, 96)     384         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_257 (Activation)     (None, 9, 9, 96)     0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 9, 9, 128)    12416       activation_257[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 9, 9, 128)    512         res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 9, 9, 128)    0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_255[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_258 (Activation)     (None, 9, 9, 128)    0           add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 9, 9, 96)     12384       activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 9, 9, 96)     384         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_259 (Activation)     (None, 9, 9, 96)     0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 9, 9, 96)     83040       activation_259[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 9, 9, 96)     384         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_260 (Activation)     (None, 9, 9, 96)     0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 9, 9, 128)    12416       activation_260[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 9, 9, 128)    512         res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 9, 9, 128)    0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_258[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_261 (Activation)     (None, 9, 9, 128)    0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 9, 9, 96)     12384       activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 9, 9, 96)     384         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_262 (Activation)     (None, 9, 9, 96)     0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 9, 9, 96)     83040       activation_262[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 9, 9, 96)     384         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_263 (Activation)     (None, 9, 9, 96)     0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 9, 9, 128)    12416       activation_263[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 9, 9, 128)    512         res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_84 (Add)                    (None, 9, 9, 128)    0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_261[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_264 (Activation)     (None, 9, 9, 128)    0           add_84[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 4, 4, 128)    147584      activation_264[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 4, 4, 128)    0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_12 (Flatten)            (None, 2048)         0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 256)          524544      flatten_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 256)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1)            257         dropout_13[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,283,841\n",
      "Trainable params: 1,279,361\n",
      "Non-trainable params: 4,480\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input,AveragePooling2D,GlobalMaxPooling2D,GlobalAveragePooling2D,Flatten\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n",
    "    And the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    bn_axis = 3\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides,\n",
    "               name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def create_model():\n",
    "    img_input = Input(shape=(75,75,3))\n",
    "    channel_axis = 3\n",
    "    bn_axis = 3\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', name='conv1')(img_input)\n",
    "    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 96], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 96], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 96], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [96, 96, 128], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [96, 96, 128], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [96, 96, 128], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [96, 96, 128], stage=3, block='d')\n",
    "    \n",
    "    x = Conv2D(128, 3, strides=2, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1,activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(img_input, x, name='inception_v3')\n",
    "print('model model')\n",
    "\n",
    "test_m = create_model()\n",
    "test_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 0.70610, saving model to best_m.h5\n",
      " - 49s - loss: 0.6652 - acc: 0.6752 - val_loss: 0.7061 - val_acc: 0.4514\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 13s - loss: 0.4006 - acc: 0.8059 - val_loss: 0.7092 - val_acc: 0.4613\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.70610 to 0.46111, saving model to best_m.h5\n",
      " - 13s - loss: 0.3469 - acc: 0.8373 - val_loss: 0.4611 - val_acc: 0.7431\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.46111 to 0.31506, saving model to best_m.h5\n",
      " - 13s - loss: 0.3032 - acc: 0.8624 - val_loss: 0.3151 - val_acc: 0.8778\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 12s - loss: 0.2870 - acc: 0.8691 - val_loss: 1.4117 - val_acc: 0.5985\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.31506 to 0.26735, saving model to best_m.h5\n",
      " - 13s - loss: 0.2635 - acc: 0.8788 - val_loss: 0.2674 - val_acc: 0.8678\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 12s - loss: 0.2321 - acc: 0.8961 - val_loss: 0.2774 - val_acc: 0.8803\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 13s - loss: 0.2343 - acc: 0.8971 - val_loss: 0.4766 - val_acc: 0.7706\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 12s - loss: 0.2315 - acc: 0.8965 - val_loss: 2.2030 - val_acc: 0.5586\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 13s - loss: 0.2120 - acc: 0.9086 - val_loss: 0.3093 - val_acc: 0.8728\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 13s - loss: 0.1900 - acc: 0.9223 - val_loss: 0.8595 - val_acc: 0.7431\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 12s - loss: 0.1609 - acc: 0.9364 - val_loss: 0.4413 - val_acc: 0.8030\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 13s - loss: 0.1639 - acc: 0.9314 - val_loss: 1.6008 - val_acc: 0.6858\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 12s - loss: 0.1684 - acc: 0.9366 - val_loss: 0.4298 - val_acc: 0.8005\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 13s - loss: 0.1458 - acc: 0.9416 - val_loss: 3.8755 - val_acc: 0.5536\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 12s - loss: 0.1355 - acc: 0.9491 - val_loss: 0.6925 - val_acc: 0.6559\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 13s - loss: 0.1201 - acc: 0.9518 - val_loss: 0.5218 - val_acc: 0.8229\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 12s - loss: 0.1185 - acc: 0.9576 - val_loss: 0.6763 - val_acc: 0.7706\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 13s - loss: 0.0816 - acc: 0.9705 - val_loss: 4.6756 - val_acc: 0.5536\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 12s - loss: 0.1036 - acc: 0.9632 - val_loss: 1.1157 - val_acc: 0.6908\n",
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 0.83939, saving model to best_m.h5\n",
      " - 56s - loss: 0.6262 - acc: 0.6962 - val_loss: 0.8394 - val_acc: 0.5287\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 12s - loss: 0.4249 - acc: 0.7961 - val_loss: 1.8334 - val_acc: 0.5287\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 13s - loss: 0.3520 - acc: 0.8346 - val_loss: 1.9213 - val_acc: 0.5287\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.83939 to 0.53021, saving model to best_m.h5\n",
      " - 13s - loss: 0.3209 - acc: 0.8525 - val_loss: 0.5302 - val_acc: 0.7257\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 12s - loss: 0.2890 - acc: 0.8662 - val_loss: 0.5781 - val_acc: 0.6733\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.53021 to 0.37829, saving model to best_m.h5\n",
      " - 12s - loss: 0.2832 - acc: 0.8705 - val_loss: 0.3783 - val_acc: 0.8404\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 13s - loss: 0.2561 - acc: 0.8859 - val_loss: 0.4250 - val_acc: 0.7756\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 13s - loss: 0.2462 - acc: 0.8886 - val_loss: 0.7558 - val_acc: 0.5586\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 13s - loss: 0.2360 - acc: 0.8965 - val_loss: 0.5582 - val_acc: 0.7706\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 12s - loss: 0.2128 - acc: 0.9034 - val_loss: 0.7689 - val_acc: 0.7182\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 12s - loss: 0.1991 - acc: 0.9123 - val_loss: 2.5776 - val_acc: 0.5287\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 12s - loss: 0.1911 - acc: 0.9177 - val_loss: 0.4554 - val_acc: 0.7905\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 12s - loss: 0.1840 - acc: 0.9179 - val_loss: 0.4844 - val_acc: 0.7955\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 12s - loss: 0.1594 - acc: 0.9308 - val_loss: 1.6276 - val_acc: 0.5511\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 12s - loss: 0.1566 - acc: 0.9358 - val_loss: 0.8351 - val_acc: 0.7805\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 13s - loss: 0.1802 - acc: 0.9219 - val_loss: 3.7431 - val_acc: 0.5287\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 12s - loss: 0.1278 - acc: 0.9491 - val_loss: 2.1971 - val_acc: 0.5586\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 12s - loss: 0.1198 - acc: 0.9503 - val_loss: 0.6020 - val_acc: 0.7955\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 12s - loss: 0.1234 - acc: 0.9516 - val_loss: 0.3925 - val_acc: 0.8229\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 13s - loss: 0.0996 - acc: 0.9622 - val_loss: 0.4675 - val_acc: 0.8529\n",
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 0.71039, saving model to best_m.h5\n",
      " - 60s - loss: 0.6391 - acc: 0.6841 - val_loss: 0.7104 - val_acc: 0.5087\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss did not improve\n",
      " - 13s - loss: 0.4225 - acc: 0.7957 - val_loss: 0.7368 - val_acc: 0.5561\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.71039 to 0.51220, saving model to best_m.h5\n",
      " - 13s - loss: 0.3622 - acc: 0.8300 - val_loss: 0.5122 - val_acc: 0.7157\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.51220 to 0.33918, saving model to best_m.h5\n",
      " - 13s - loss: 0.3126 - acc: 0.8510 - val_loss: 0.3392 - val_acc: 0.8354\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.33918 to 0.33271, saving model to best_m.h5\n",
      " - 13s - loss: 0.2997 - acc: 0.8616 - val_loss: 0.3327 - val_acc: 0.8579\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 13s - loss: 0.2691 - acc: 0.8834 - val_loss: 0.4101 - val_acc: 0.8130\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 12s - loss: 0.2500 - acc: 0.8940 - val_loss: 0.4435 - val_acc: 0.7880\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 12s - loss: 0.2459 - acc: 0.8965 - val_loss: 1.1959 - val_acc: 0.5636\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 13s - loss: 0.2143 - acc: 0.9086 - val_loss: 0.3678 - val_acc: 0.8504\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 12s - loss: 0.1990 - acc: 0.9183 - val_loss: 0.6170 - val_acc: 0.6559\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 13s - loss: 0.2116 - acc: 0.9163 - val_loss: 0.4838 - val_acc: 0.7731\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 13s - loss: 0.1635 - acc: 0.9308 - val_loss: 0.4407 - val_acc: 0.7731\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 13s - loss: 0.1763 - acc: 0.9277 - val_loss: 0.3435 - val_acc: 0.8479\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 12s - loss: 0.1574 - acc: 0.9426 - val_loss: 4.4630 - val_acc: 0.5062\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 13s - loss: 0.1418 - acc: 0.9408 - val_loss: 3.2080 - val_acc: 0.5162\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 13s - loss: 0.1425 - acc: 0.9456 - val_loss: 0.4118 - val_acc: 0.8504\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 13s - loss: 0.1070 - acc: 0.9589 - val_loss: 1.2429 - val_acc: 0.7506\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 13s - loss: 0.1153 - acc: 0.9547 - val_loss: 0.3395 - val_acc: 0.8753\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 12s - loss: 0.1065 - acc: 0.9601 - val_loss: 0.8954 - val_acc: 0.7581\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 12s - loss: 0.0997 - acc: 0.9659 - val_loss: 0.3371 - val_acc: 0.8853\n",
      "1203\n",
      "Train on 4812 samples, validate on 401 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 0.68693, saving model to best_m.h5\n",
      " - 67s - loss: 0.6409 - acc: 0.6820 - val_loss: 0.6869 - val_acc: 0.5362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 0.68693 to 0.68515, saving model to best_m.h5\n",
      " - 13s - loss: 0.4155 - acc: 0.8022 - val_loss: 0.6852 - val_acc: 0.5362\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss did not improve\n",
      " - 13s - loss: 0.3268 - acc: 0.8495 - val_loss: 0.8829 - val_acc: 0.5761\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.68515 to 0.41219, saving model to best_m.h5\n",
      " - 13s - loss: 0.2931 - acc: 0.8674 - val_loss: 0.4122 - val_acc: 0.8080\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 13s - loss: 0.2682 - acc: 0.8757 - val_loss: 0.8438 - val_acc: 0.6309\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 13s - loss: 0.2424 - acc: 0.8899 - val_loss: 0.4956 - val_acc: 0.8279\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 13s - loss: 0.2166 - acc: 0.8984 - val_loss: 3.1656 - val_acc: 0.5461\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 13s - loss: 0.2024 - acc: 0.9050 - val_loss: 1.3134 - val_acc: 0.5761\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 13s - loss: 0.1785 - acc: 0.9217 - val_loss: 0.9102 - val_acc: 0.5910\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 13s - loss: 0.1756 - acc: 0.9217 - val_loss: 1.2770 - val_acc: 0.6683\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.41219 to 0.37529, saving model to best_m.h5\n",
      " - 13s - loss: 0.1764 - acc: 0.9214 - val_loss: 0.3753 - val_acc: 0.8504\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 13s - loss: 0.1739 - acc: 0.9296 - val_loss: 0.3928 - val_acc: 0.8479\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 13s - loss: 0.1573 - acc: 0.9343 - val_loss: 0.4188 - val_acc: 0.7930\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 13s - loss: 0.1426 - acc: 0.9447 - val_loss: 3.6692 - val_acc: 0.5586\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 13s - loss: 0.1443 - acc: 0.9433 - val_loss: 0.8543 - val_acc: 0.7082\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 13s - loss: 0.1246 - acc: 0.9541 - val_loss: 1.6572 - val_acc: 0.6334\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 13s - loss: 0.1345 - acc: 0.9497 - val_loss: 1.1929 - val_acc: 0.6733\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 13s - loss: 0.1138 - acc: 0.9568 - val_loss: 0.3939 - val_acc: 0.8304\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 13s - loss: 0.1042 - acc: 0.9566 - val_loss: 0.8188 - val_acc: 0.7930\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 13s - loss: 0.0925 - acc: 0.9682 - val_loss: 0.4475 - val_acc: 0.7581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kfold_train(fold_cnt=3,rnd=233):\n",
    "    train_pred, test_pred = np.zeros((1604,1)),np.zeros((8424,1))\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=True, random_state=2*rnd)\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        curr_x,curr_y = train_x[train_index],y[train_index]\n",
    "        curr_x,curr_y = rot_aut(curr_x,curr_y)\n",
    "        val_x,val_y = train_x[test_index],y[test_index]\n",
    "        \n",
    "        model = create_model()\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(0.0005), metrics=['accuracy'])\n",
    "        model_p = 'best_m.h5'\n",
    "        model_chk = ModelCheckpoint(filepath=model_p, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        model.fit(curr_x,curr_y,\n",
    "                  validation_data=(val_x,val_y),\n",
    "                  batch_size=32, epochs=20, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk]\n",
    "                 )\n",
    "        model = load_model(model_p)\n",
    "        \n",
    "        train_pred[test_index] = model.predict(val_x)\n",
    "        test_pred = test_pred + model.predict(test_x)/fold_cnt\n",
    "    return train_pred,test_pred\n",
    "\n",
    "train_pred,test_pred = kfold_train(fold_cnt=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.338411347687\n",
      "         id  is_iceberg\n",
      "0  5941774d    0.370994\n",
      "1  4023181e    0.159682\n",
      "2  b20200e4    0.050549\n",
      "3  e7f018bb    0.993832\n",
      "4  4371c8c3    0.174863\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../features/resnet_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "\n",
    "# train feat loss\n",
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y,train_pred))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test_df['id']\n",
    "submission['is_iceberg']=test_pred\n",
    "print(submission.head())\n",
    "submission.to_csv('../results/resnet_1_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
